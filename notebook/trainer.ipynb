{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 算法模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "\n",
    "import trainer.constants as cst\n",
    "import trainer.data_pipeline as dp\n",
    "import trainer.split_model as split_model\n",
    "import trainer.full_cnn_model as full_cnn_model\n",
    "from trainer.callbacks import CustomCheckpoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 导入模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    \"\"\"Argument parser.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        type=str,\n",
    "        default=cst.BASE_DIR,\n",
    "        help='local or GCS location for writing checkpoints and exporting models')\n",
    "    parser.add_argument(\n",
    "        '--data-dir-train',\n",
    "        type=str,\n",
    "        default=cst.TRAIN_SET,\n",
    "        help='local or GCS location for reading TFRecord files for the training set')\n",
    "    parser.add_argument(\n",
    "        '--data-dir-validate',\n",
    "        type=str,\n",
    "        default=cst.TEST_SET,\n",
    "        help='local or GCS location for reading TFRecord files for the validation set')\n",
    "    parser.add_argument(\n",
    "        '--tboard-dir',         # no default so we can construct dynamically with timestamp\n",
    "        type=str,\n",
    "        help='local or GCS location for reading TensorBoard files')\n",
    "    parser.add_argument(\n",
    "        '--saved-model-dir',    # no default so we can construct dynamically with timestamp\n",
    "        type=str,\n",
    "        help='local or GCS location for saving trained Keras models')\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help='number of times to go through the data, default=3')\n",
    "    parser.add_argument(\n",
    "        '--batch-size',\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help='number of records to read during each training step, default=16')\n",
    "    parser.add_argument(\n",
    "        '--window-size',\n",
    "        default=20,\n",
    "        type=int,\n",
    "        help='window size for sliding window in training sample generation, default=100')\n",
    "    parser.add_argument(\n",
    "        '--shift',\n",
    "        default=5,\n",
    "        type=int,\n",
    "        help='shift for sliding window in training sample generation, default=20')\n",
    "    parser.add_argument(\n",
    "        '--stride',\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help='stride inside sliding window in training sample generation, default=1')\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "        default='DEBUG')\n",
    "    parser.add_argument(\n",
    "        '--loss',\n",
    "        default='mean_squared_error',\n",
    "        type=str,\n",
    "        help='loss function used by the model, default=mean_squared_error')\n",
    "    parser.add_argument(\n",
    "        '--shuffle',\n",
    "        default=True,\n",
    "        type=bool,\n",
    "        help='shuffle the batched dataset, default=True'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--shuffle-buffer',\n",
    "        default=5,\n",
    "        type=int,\n",
    "        help='Bigger buffer size means better shuffling but longer setup time. Default=500'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--save-from',\n",
    "        default=80,\n",
    "        type=int,\n",
    "        help='epoch after which model checkpoints are saved, default=80'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--model',\n",
    "        default='split_model',\n",
    "        type=str,\n",
    "        help='The type of model to use, default=\"split_model\", options=\"split_model\", \"full_cnn_model\"'\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def train_and_evaluate(args, tboard_dir, hparams=None):\n",
    "    \"\"\"Trains and evaluates the Keras model.\n",
    "\n",
    "    Uses the Keras model defined in model.py and trains on data loaded and\n",
    "    preprocessed in data_pipeline.py. Saves the trained model in TensorFlow SavedModel\n",
    "    format to the path defined in part by the --job-dir argument.\n",
    "\n",
    "    Args:\n",
    "    args: dictionary of arguments - see get_args() for details\n",
    "    \"\"\"\n",
    "    # Config datasets for consistent usage\n",
    "    ds_config = dict(window_size=args.window_size,\n",
    "                     shift=args.shift,\n",
    "                     stride=args.stride,\n",
    "                     batch_size=args.batch_size)\n",
    "    ds_train_path = args.data_dir_train\n",
    "    ds_val_path = args.data_dir_validate\n",
    "\n",
    "    # create model\n",
    "    if args.model == 'split_model':\n",
    "        print(\"Using split model!\")\n",
    "        model = split_model.create_keras_model(window_size=ds_config[\"window_size\"],\n",
    "                                               loss=args.loss,\n",
    "                                               hparams_config=hparams)\n",
    "    if args.model == 'full_cnn_model':\n",
    "        print(\"Using full cnn model!\")\n",
    "        model = full_cnn_model.create_keras_model(window_size=ds_config[\"window_size\"],\n",
    "                                                  loss=args.loss,\n",
    "                                                  hparams_config=hparams)\n",
    "\n",
    "    # Calculate steps_per_epoch_train, steps_per_epoch_test\n",
    "    # This is needed, since for counting repeat has to be false\n",
    "    steps_per_epoch_train = calculate_steps_per_epoch(data_dir=ds_train_path, dataset_config=ds_config)\n",
    "\n",
    "    steps_per_epoch_validate = calculate_steps_per_epoch(data_dir=ds_val_path, dataset_config=ds_config)\n",
    "\n",
    "    # load datasets\n",
    "    dataset_train = dp.create_dataset(data_dir=ds_train_path,\n",
    "                                      window_size=ds_config[\"window_size\"],\n",
    "                                      shift=ds_config[\"shift\"],\n",
    "                                      stride=ds_config[\"stride\"],\n",
    "                                      batch_size=ds_config[\"batch_size\"])\n",
    "\n",
    "    dataset_validate = dp.create_dataset(data_dir=ds_val_path,\n",
    "                                         window_size=ds_config[\"window_size\"],\n",
    "                                         shift=ds_config[\"shift\"],\n",
    "                                         stride=ds_config[\"stride\"],\n",
    "                                         batch_size=ds_config[\"batch_size\"])\n",
    "\n",
    "    # if hparams is passed, we're running a HPO-job\n",
    "    if hparams:\n",
    "        checkpoint_callback = CustomCheckpoints(save_last_only=True,\n",
    "                                                log_dir=tboard_dir,\n",
    "                                                dataset_path=ds_val_path,\n",
    "                                                dataset_config=ds_config,\n",
    "                                                save_eval_plot=False)\n",
    "    else:\n",
    "        checkpoint_callback = CustomCheckpoints(save_best_only=True,\n",
    "                                                start_epoch=args.save_from,\n",
    "                                                log_dir=tboard_dir,\n",
    "                                                dataset_path=ds_val_path,\n",
    "                                                dataset_config=ds_config,\n",
    "                                                save_eval_plot=False)\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=tboard_dir,\n",
    "                                       histogram_freq=0,\n",
    "                                       write_graph=False,\n",
    "                                       ),\n",
    "        checkpoint_callback,\n",
    "    ]\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # train model\n",
    "    history = model.fit(\n",
    "        dataset_train,\n",
    "        epochs=args.num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch_train,\n",
    "        validation_data=dataset_validate,\n",
    "        validation_steps=steps_per_epoch_validate,\n",
    "        verbose=2,\n",
    "        callbacks=callbacks)\n",
    "\n",
    "    mae_current = min(history.history[\"val_mae_current_cycle\"])\n",
    "    mae_remaining = min(history.history[\"val_mae_remaining_cycles\"])\n",
    "    print('\\nhistory dict:', history.history)\n",
    "    return mae_current, mae_remaining\n",
    "\n",
    "\n",
    "def calculate_steps_per_epoch(data_dir, dataset_config):\n",
    "    temp_dataset = dp.create_dataset(data_dir=data_dir,\n",
    "                                     window_size=dataset_config[\"window_size\"],\n",
    "                                     shift=dataset_config[\"shift\"],\n",
    "                                     stride=dataset_config[\"stride\"],\n",
    "                                     batch_size=dataset_config[\"batch_size\"],\n",
    "                                     repeat=False)\n",
    "    steps_per_epoch = 0\n",
    "    for batch in temp_dataset:\n",
    "        steps_per_epoch += 1\n",
    "    return steps_per_epoch\n",
    "\n",
    "\n",
    "def get_tboard_dir():\n",
    "    run_timestr = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    if args.tboard_dir is None:\n",
    "        tboard_dir = os.path.join(cst.TENSORBOARD_DIR, \"jobs\", run_timestr)\n",
    "    else:\n",
    "        tboard_dir = args.tboard_dir\n",
    "    return tboard_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 启动训练脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    logging.set_verbosity(args.verbosity)\n",
    "    train_and_evaluate(args, get_tboard_dir())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型预测\n",
    "### 加载预测及可视化函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 提取出训练得到的模型，请将下面的文件更换为\"SavedModel written to：\"后的地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!cp -r Graph/jobs/20200604-064509/checkpoints/last_epoch_loss_0.026658709228579557/ saved_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from trainer.plot import plot_single_prediction\n",
    "from trainer.custom_metrics_losses import mae_current_cycle, mae_remaining_cycles\n",
    "from trainer.clippy import Clippy, clipped_relu\n",
    "\n",
    "def make_plot(model, predictions):\n",
    "    predictions = np.array(predictions)\n",
    "    first_pred = predictions[0]\n",
    "    window_size = model.input_shape[0][1]\n",
    "    # this number comes from training dataset preprocessing\n",
    "    scaling_factors_dict = {\"Remaining_cycles\": 2159.0}\n",
    "    mean_cycle_life = 674  # calculated from training set\n",
    "    figure = plot_single_prediction(first_pred,\n",
    "                                  window_size,\n",
    "                                  scaling_factors_dict,\n",
    "                                  mean_cycle_life)\n",
    "    return figure\n",
    "\n",
    "def make_prediction(model, cycle_data):\n",
    "    cycles = { 'Qdlin': np.array(json.loads(cycle_data['Qdlin'])),\n",
    "                'Tdlin': np.array(json.loads(cycle_data['Tdlin'])),\n",
    "                'IR': np.array(json.loads(cycle_data['IR'])),\n",
    "                'Discharge_time': np.array(json.loads(cycle_data['Discharge_time'])),\n",
    "                'QD': np.array(json.loads(cycle_data['QD']))\n",
    "            }\n",
    "\n",
    "    predictions = model.predict(cycles)\n",
    "    print_predictions = model.predict(cycles)\n",
    "    print_predictions[0] = print_predictions[0] * 2159\n",
    "    print(\"Returning predictions:\")\n",
    "    print(print_predictions)\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from trainer.plot import plot_single_prediction\n",
    "from trainer.custom_metrics_losses import mae_current_cycle, mae_remaining_cycles\n",
    "from trainer.clippy import Clippy, clipped_relu\n",
    "\n",
    "def make_plot(model, predictions):\n",
    "    predictions = np.array(predictions)\n",
    "    first_pred = predictions[0]\n",
    "    window_size = model.input_shape[0][1]\n",
    "    # this number comes from training dataset preprocessing\n",
    "    scaling_factors_dict = {\"Remaining_cycles\": 2159.0}\n",
    "    mean_cycle_life = 674  # calculated from training set\n",
    "    figure = plot_single_prediction(first_pred,\n",
    "                                  window_size,\n",
    "                                  scaling_factors_dict,\n",
    "                                  mean_cycle_life)\n",
    "    return figure\n",
    "\n",
    "def make_prediction(model, cycle_data):\n",
    "    cycles = { 'Qdlin': np.array(json.loads(cycle_data['Qdlin'])),\n",
    "                'Tdlin': np.array(json.loads(cycle_data['Tdlin'])),\n",
    "                'IR': np.array(json.loads(cycle_data['IR'])),\n",
    "                'Discharge_time': np.array(json.loads(cycle_data['Discharge_time'])),\n",
    "                'QD': np.array(json.loads(cycle_data['QD']))\n",
    "            }\n",
    "\n",
    "    predictions = model.predict(cycles)\n",
    "    print_predictions = model.predict(cycles)\n",
    "    print_predictions[0] = print_predictions[0] * 2159\n",
    "    print(\"Returning predictions:\")\n",
    "    print(print_predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from trainer.plot import plot_single_prediction\n",
    "from trainer.custom_metrics_losses import mae_current_cycle, mae_remaining_cycles\n",
    "from trainer.clippy import Clippy, clipped_relu\n",
    "\n",
    "def make_plot(model, predictions):\n",
    "    predictions = np.array(predictions)\n",
    "    first_pred = predictions[0]\n",
    "    window_size = model.input_shape[0][1]\n",
    "    # this number comes from training dataset preprocessing\n",
    "    scaling_factors_dict = {\"Remaining_cycles\": 2159.0}\n",
    "    mean_cycle_life = 674  # calculated from training set\n",
    "    figure = plot_single_prediction(first_pred,\n",
    "                                  window_size,\n",
    "                                  scaling_factors_dict,\n",
    "                                  mean_cycle_life)\n",
    "    return figure\n",
    "\n",
    "def make_prediction(model, cycle_data):\n",
    "    cycles = { 'Qdlin': np.array(json.loads(cycle_data['Qdlin'])),\n",
    "                'Tdlin': np.array(json.loads(cycle_data['Tdlin'])),\n",
    "                'IR': np.array(json.loads(cycle_data['IR'])),\n",
    "                'Discharge_time': np.array(json.loads(cycle_data['Discharge_time'])),\n",
    "                'QD': np.array(json.loads(cycle_data['QD']))\n",
    "            }\n",
    "\n",
    "    predictions = model.predict(cycles)\n",
    "    print_predictions = model.predict(cycles)\n",
    "    print_predictions[0] = print_predictions[0] * 2159\n",
    "    print(\"Returning predictions:\")\n",
    "    print(print_predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 导入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = \"saved_model/\"\n",
    "# model = tf.keras.models.load_model(MODEL_DIR, custom_objects={'clippy': Clippy(clipped_relu),\n",
    "#                                                               'mae_current_cycle': mae_current_cycle,\n",
    "#                                                               'mae_remaining_cycles': mae_remaining_cycles})\n",
    "#\n",
    "model = tf.keras.experimental.load_from_saved_model(MODEL_DIR,\n",
    "                                       custom_objects={'clippy': Clippy(clipped_relu),\n",
    "                                                       'mae_current_cycle': mae_current_cycle,\n",
    "                                                       'mae_remaining_cycles': mae_remaining_cycles})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 从测试集生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import trainer.constants as cst\n",
    "from trainer.data_pipeline import create_dataset\n",
    "\n",
    "\"\"\"Create sample files in json format from test data and save it in the server module.\n",
    "These can be used by the 'load random sample' button as examples on the website.\n",
    "\"\"\"\n",
    "\n",
    "samples_fullpath = 'test_samples/'\n",
    "NUM_SAMPLES = 6\n",
    "\n",
    "if not os.path.exists(samples_fullpath):\n",
    "    os.makedirs(samples_fullpath)\n",
    "\n",
    "dataset = create_dataset(cst.SECONDARY_TEST_SET,\n",
    "                         window_size=20,\n",
    "                         shift=1,\n",
    "                         stride=1,\n",
    "                         batch_size=1)\n",
    "rows = dataset.take(NUM_SAMPLES)\n",
    "for i, row in enumerate(rows):\n",
    "    sample = {key: str(value.numpy().tolist()) for key, value in row[0].items()}\n",
    "    with open(os.path.join(samples_fullpath, 'sample_input_{}.json'.format(i+1)), 'w') as outfile:\n",
    "        json.dump(sample, outfile)\n",
    "print(\"Created {} sample files in test_samples\".format(NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 运用模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "json_file_sample = './test_samples/sample_input_4.json'\n",
    "\n",
    "with open(json_file_sample, 'r') as infd:\n",
    "  json_data = json.load(infd)\n",
    "#print(json_data)\n",
    "predictions_soh_rul= make_prediction(model, json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "figure = make_plot(model, predictions_soh_rul)\n",
    "pyo.plot(figure,filename='Prediction.html')\n",
    "print('You can click the Prediction.html in the left sidebar to see the prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}