{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 算法模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "\n",
    "import trainer.constants as cst\n",
    "import trainer.data_pipeline as dp\n",
    "import trainer.split_model as split_model\n",
    "import trainer.full_cnn_model as full_cnn_model\n",
    "from trainer.callbacks import CustomCheckpoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 导入模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    \"\"\"Argument parser.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        type=str,\n",
    "        default=cst.BASE_DIR,\n",
    "        help='local or GCS location for writing checkpoints and exporting models')\n",
    "    parser.add_argument(\n",
    "        '--data-dir-train',\n",
    "        type=str,\n",
    "        default=cst.TRAIN_SET,\n",
    "        help='local or GCS location for reading TFRecord files for the training set')\n",
    "    parser.add_argument(\n",
    "        '--data-dir-validate',\n",
    "        type=str,\n",
    "        default=cst.TEST_SET,\n",
    "        help='local or GCS location for reading TFRecord files for the validation set')\n",
    "    parser.add_argument(\n",
    "        '--tboard-dir',         # no default so we can construct dynamically with timestamp\n",
    "        type=str,\n",
    "        help='local or GCS location for reading TensorBoard files')\n",
    "    parser.add_argument(\n",
    "        '--saved-model-dir',    # no default so we can construct dynamically with timestamp\n",
    "        type=str,\n",
    "        help='local or GCS location for saving trained Keras models')\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help='number of times to go through the data, default=3')\n",
    "    parser.add_argument(\n",
    "        '--batch-size',\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help='number of records to read during each training step, default=16')\n",
    "    parser.add_argument(\n",
    "        '--window-size',\n",
    "        default=20,\n",
    "        type=int,\n",
    "        help='window size for sliding window in training sample generation, default=100')\n",
    "    parser.add_argument(\n",
    "        '--shift',\n",
    "        default=5,\n",
    "        type=int,\n",
    "        help='shift for sliding window in training sample generation, default=20')\n",
    "    parser.add_argument(\n",
    "        '--stride',\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help='stride inside sliding window in training sample generation, default=1')\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "        default='DEBUG')\n",
    "    parser.add_argument(\n",
    "        '--loss',\n",
    "        default='mean_squared_error',\n",
    "        type=str,\n",
    "        help='loss function used by the model, default=mean_squared_error')\n",
    "    parser.add_argument(\n",
    "        '--shuffle',\n",
    "        default=True,\n",
    "        type=bool,\n",
    "        help='shuffle the batched dataset, default=True'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--shuffle-buffer',\n",
    "        default=5,\n",
    "        type=int,\n",
    "        help='Bigger buffer size means better shuffling but longer setup time. Default=500'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--save-from',\n",
    "        default=80,\n",
    "        type=int,\n",
    "        help='epoch after which model checkpoints are saved, default=80'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--model',\n",
    "        default='split_model',\n",
    "        type=str,\n",
    "        help='The type of model to use, default=\"split_model\", options=\"split_model\", \"full_cnn_model\"'\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def train_and_evaluate(args, tboard_dir, hparams=None):\n",
    "    \"\"\"Trains and evaluates the Keras model.\n",
    "\n",
    "    Uses the Keras model defined in model.py and trains on data loaded and\n",
    "    preprocessed in data_pipeline.py. Saves the trained model in TensorFlow SavedModel\n",
    "    format to the path defined in part by the --job-dir argument.\n",
    "\n",
    "    Args:\n",
    "    args: dictionary of arguments - see get_args() for details\n",
    "    \"\"\"\n",
    "    # Config datasets for consistent usage\n",
    "    ds_config = dict(window_size=args.window_size,\n",
    "                     shift=args.shift,\n",
    "                     stride=args.stride,\n",
    "                     batch_size=args.batch_size)\n",
    "    ds_train_path = args.data_dir_train\n",
    "    ds_val_path = args.data_dir_validate\n",
    "\n",
    "    # create model\n",
    "    if args.model == 'split_model':\n",
    "        print(\"Using split model!\")\n",
    "        model = split_model.create_keras_model(window_size=ds_config[\"window_size\"],\n",
    "                                               loss=args.loss,\n",
    "                                               hparams_config=hparams)\n",
    "    if args.model == 'full_cnn_model':\n",
    "        print(\"Using full cnn model!\")\n",
    "        model = full_cnn_model.create_keras_model(window_size=ds_config[\"window_size\"],\n",
    "                                                  loss=args.loss,\n",
    "                                                  hparams_config=hparams)\n",
    "\n",
    "    # Calculate steps_per_epoch_train, steps_per_epoch_test\n",
    "    # This is needed, since for counting repeat has to be false\n",
    "    steps_per_epoch_train = calculate_steps_per_epoch(data_dir=ds_train_path, dataset_config=ds_config)\n",
    "\n",
    "    steps_per_epoch_validate = calculate_steps_per_epoch(data_dir=ds_val_path, dataset_config=ds_config)\n",
    "\n",
    "    # load datasets\n",
    "    dataset_train = dp.create_dataset(data_dir=ds_train_path,\n",
    "                                      window_size=ds_config[\"window_size\"],\n",
    "                                      shift=ds_config[\"shift\"],\n",
    "                                      stride=ds_config[\"stride\"],\n",
    "                                      batch_size=ds_config[\"batch_size\"])\n",
    "\n",
    "    dataset_validate = dp.create_dataset(data_dir=ds_val_path,\n",
    "                                         window_size=ds_config[\"window_size\"],\n",
    "                                         shift=ds_config[\"shift\"],\n",
    "                                         stride=ds_config[\"stride\"],\n",
    "                                         batch_size=ds_config[\"batch_size\"])\n",
    "\n",
    "    # if hparams is passed, we're running a HPO-job\n",
    "    if hparams:\n",
    "        checkpoint_callback = CustomCheckpoints(save_last_only=True,\n",
    "                                                log_dir=tboard_dir,\n",
    "                                                dataset_path=ds_val_path,\n",
    "                                                dataset_config=ds_config,\n",
    "                                                save_eval_plot=False)\n",
    "    else:\n",
    "        checkpoint_callback = CustomCheckpoints(save_best_only=True,\n",
    "                                                start_epoch=args.save_from,\n",
    "                                                log_dir=tboard_dir,\n",
    "                                                dataset_path=ds_val_path,\n",
    "                                                dataset_config=ds_config,\n",
    "                                                save_eval_plot=False)\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=tboard_dir,\n",
    "                                       histogram_freq=0,\n",
    "                                       write_graph=False,\n",
    "                                       ),\n",
    "        checkpoint_callback,\n",
    "    ]\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # train model\n",
    "    history = model.fit(\n",
    "        dataset_train,\n",
    "        epochs=args.num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch_train,\n",
    "        validation_data=dataset_validate,\n",
    "        validation_steps=steps_per_epoch_validate,\n",
    "        verbose=2,\n",
    "        callbacks=callbacks)\n",
    "\n",
    "    mae_current = min(history.history[\"val_mae_current_cycle\"])\n",
    "    mae_remaining = min(history.history[\"val_mae_remaining_cycles\"])\n",
    "    print('\\nhistory dict:', history.history)\n",
    "    return mae_current, mae_remaining\n",
    "\n",
    "\n",
    "def calculate_steps_per_epoch(data_dir, dataset_config):\n",
    "    temp_dataset = dp.create_dataset(data_dir=data_dir,\n",
    "                                     window_size=dataset_config[\"window_size\"],\n",
    "                                     shift=dataset_config[\"shift\"],\n",
    "                                     stride=dataset_config[\"stride\"],\n",
    "                                     batch_size=dataset_config[\"batch_size\"],\n",
    "                                     repeat=False)\n",
    "    steps_per_epoch = 0\n",
    "    for batch in temp_dataset:\n",
    "        steps_per_epoch += 1\n",
    "    return steps_per_epoch\n",
    "\n",
    "\n",
    "def get_tboard_dir():\n",
    "    run_timestr = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    if args.tboard_dir is None:\n",
    "        tboard_dir = os.path.join(cst.TENSORBOARD_DIR, \"jobs\", run_timestr)\n",
    "    else:\n",
    "        tboard_dir = args.tboard_dir\n",
    "    return tboard_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 启动训练脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using split model!\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Qdlin (InputLayer)              [(None, 20, 1000, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Tdlin (InputLayer)              [(None, 20, 1000, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "detail_concat (Concatenate)     (None, 20, 1000, 2)  0           Qdlin[0][0]                      \n",
      "                                                                 Tdlin[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "convolution (TimeDistributed)   (None, 20, 334, 32)  608         detail_concat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pool (TimeDistributed)     (None, 20, 167, 32)  0           convolution[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (TimeDistributed)         (None, 20, 56, 64)   18496       conv_pool[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (TimeDistributed)         (None, 20, 28, 64)   0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (TimeDistributed)         (None, 20, 10, 128)  73856       pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (TimeDistributed)         (None, 20, 5, 128)   0           conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "convolution_flat (TimeDistribut (None, 20, 640)      0           pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_cnn (TimeDistributed)   (None, 20, 640)      0           convolution_flat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "IR (InputLayer)                 [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Discharge_time (InputLayer)     [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QD (InputLayer)                 [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "all_concat (Concatenate)        (None, 20, 643)      0           dropout_cnn[0][0]                \n",
      "                                                                 IR[0][0]                         \n",
      "                                                                 Discharge_time[0][0]             \n",
      "                                                                 QD[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "recurrent (LSTM)                (None, 128)          395264      all_concat[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_lstm (Dropout)          (None, 128)          0           recurrent[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hidden (Dense)                  (None, 32)           4128        dropout_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            66          hidden[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 492,418\n",
      "Trainable params: 492,418\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/3\n",
      "168/168 - 37s - loss: 0.0312 - mae_current_cycle: 300.6631 - mae_remaining_cycles: 200.5382 - val_loss: 0.0274 - val_mae_current_cycle: 264.1417 - val_mae_remaining_cycles: 173.2992\n",
      "Epoch 2/3\n",
      "168/168 - 32s - loss: 0.0186 - mae_current_cycle: 228.7274 - mae_remaining_cycles: 157.7123 - val_loss: 0.0264 - val_mae_current_cycle: 260.8416 - val_mae_remaining_cycles: 153.7114\n",
      "Epoch 3/3\n",
      "168/168 - 32s - loss: 0.0171 - mae_current_cycle: 228.6347 - mae_remaining_cycles: 138.7604 - val_loss: 0.0267 - val_mae_current_cycle: 262.8626 - val_mae_remaining_cycles: 159.7904\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:253: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: ['train']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: ['eval']\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: Graph/jobs/20200604-064509/checkpoints/last_epoch_loss_0.026658709228579557/saved_model.pb\n",
      "\n",
      "history dict: {'loss': [0.03120968656315069, 0.018556027216531913, 0.01705325718198548], 'mae_current_cycle': [300.66312, 228.72737, 228.63474], 'mae_remaining_cycles': [200.53816, 157.71228, 138.76042], 'val_loss': [0.02737576978140941, 0.026361836070415152, 0.026658709228579557], 'val_mae_current_cycle': [264.14172, 260.84164, 262.86264], 'val_mae_remaining_cycles': [173.29924, 153.71138, 159.79039]}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    logging.set_verbosity(args.verbosity)\n",
    "    train_and_evaluate(args, get_tboard_dir())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型预测\n",
    "### 加载预测及可视化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from trainer.plot import plot_single_prediction\n",
    "from trainer.custom_metrics_losses import mae_current_cycle, mae_remaining_cycles\n",
    "from trainer.clippy import Clippy, clipped_relu\n",
    "\n",
    "def make_plot(model, predictions):\n",
    "    predictions = np.array(predictions)\n",
    "    first_pred = predictions[0]\n",
    "    window_size = model.input_shape[0][1]\n",
    "    # this number comes from training dataset preprocessing\n",
    "    scaling_factors_dict = {\"Remaining_cycles\": 2159.0}\n",
    "    mean_cycle_life = 674  # calculated from training set\n",
    "    figure = plot_single_prediction(first_pred,\n",
    "                                  window_size,\n",
    "                                  scaling_factors_dict,\n",
    "                                  mean_cycle_life)\n",
    "    return figure\n",
    "\n",
    "def make_prediction(model, cycle_data):\n",
    "    cycles = { 'Qdlin': np.array(json.loads(cycle_data['Qdlin'])),\n",
    "                'Tdlin': np.array(json.loads(cycle_data['Tdlin'])),\n",
    "                'IR': np.array(json.loads(cycle_data['IR'])),\n",
    "                'Discharge_time': np.array(json.loads(cycle_data['Discharge_time'])),\n",
    "                'QD': np.array(json.loads(cycle_data['QD']))\n",
    "            }\n",
    "\n",
    "    predictions = model.predict(cycles)\n",
    "    print_predictions = model.predict(cycles)\n",
    "    print_predictions[0] = print_predictions[0] * 2159\n",
    "    print(\"Returning predictions:\")\n",
    "    print(print_predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 导入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = \"saved_model/\"\n",
    "# model = tf.keras.models.load_model(MODEL_DIR, custom_objects={'clippy': Clippy(clipped_relu),\n",
    "#                                                               'mae_current_cycle': mae_current_cycle,\n",
    "#                                                               'mae_remaining_cycles': mae_remaining_cycles})\n",
    "#\n",
    "model = tf.keras.experimental.load_from_saved_model(MODEL_DIR,\n",
    "                                       custom_objects={'clippy': Clippy(clipped_relu),\n",
    "                                                       'mae_current_cycle': mae_current_cycle,\n",
    "                                                       'mae_remaining_cycles': mae_remaining_cycles})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 从测试集生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "Created 6 sample files in test_samples\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import trainer.constants as cst\n",
    "from trainer.data_pipeline import create_dataset\n",
    "\n",
    "\"\"\"Create sample files in json format from test data and save it in the server module.\n",
    "These can be used by the 'load random sample' button as examples on the website.\n",
    "\"\"\"\n",
    "\n",
    "samples_fullpath = 'test_samples/'\n",
    "NUM_SAMPLES = 6\n",
    "\n",
    "if not os.path.exists(samples_fullpath):\n",
    "    os.makedirs(samples_fullpath)\n",
    "\n",
    "dataset = create_dataset(cst.SECONDARY_TEST_SET,\n",
    "                         window_size=20,\n",
    "                         shift=1,\n",
    "                         stride=1,\n",
    "                         batch_size=1)\n",
    "rows = dataset.take(NUM_SAMPLES)\n",
    "for i, row in enumerate(rows):\n",
    "    sample = {key: str(value.numpy().tolist()) for key, value in row[0].items()}\n",
    "    with open(os.path.join(samples_fullpath, 'sample_input_{}.json'.format(i+1)), 'w') as outfile:\n",
    "        json.dump(sample, outfile)\n",
    "print(\"Created {} sample files in test_samples\".format(NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 运用模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning predictions:\n",
      "[[106.15377 685.337  ]]\n"
     ]
    }
   ],
   "source": [
    "json_file_sample = './test_samples/sample_input_4.json'\n",
    "\n",
    "with open(json_file_sample, 'r') as infd:\n",
    "  json_data = json.load(infd)\n",
    "#print(json_data)\n",
    "predictions_soh_rul= make_prediction(model, json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can click the Prediction.html in the left sidebar to see the prediction\n"
     ]
    }
   ],
   "source": [
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "figure = make_plot(model, predictions_soh_rul)\n",
    "pyo.plot(figure,filename='Prediction.html')\n",
    "print('You can click the Prediction.html in the left sidebar to see the prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
