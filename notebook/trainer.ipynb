{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 算法模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "\n",
    "import trainer.constants as cst\n",
    "import trainer.data_pipeline as dp\n",
    "import trainer.split_model as split_model\n",
    "import trainer.full_cnn_model as full_cnn_model\n",
    "from trainer.callbacks import CustomCheckpoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 导入模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    \"\"\"Argument parser.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        type=str,\n",
    "        default=cst.BASE_DIR,\n",
    "        help='local or GCS location for writing checkpoints and exporting models')\n",
    "    parser.add_argument(\n",
    "        '--data-dir-train',\n",
    "        type=str,\n",
    "        default=cst.TRAIN_SET,\n",
    "        help='local or GCS location for reading TFRecord files for the training set')\n",
    "    parser.add_argument(\n",
    "        '--data-dir-validate',\n",
    "        type=str,\n",
    "        default=cst.TEST_SET,\n",
    "        help='local or GCS location for reading TFRecord files for the validation set')\n",
    "    parser.add_argument(\n",
    "        '--tboard-dir',         # no default so we can construct dynamically with timestamp\n",
    "        type=str,\n",
    "        help='local or GCS location for reading TensorBoard files')\n",
    "    parser.add_argument(\n",
    "        '--saved-model-dir',    # no default so we can construct dynamically with timestamp\n",
    "        type=str,\n",
    "        help='local or GCS location for saving trained Keras models')\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        type=int,\n",
    "        default=5,\n",
    "        help='number of times to go through the data, default=3')\n",
    "    parser.add_argument(\n",
    "        '--batch-size',\n",
    "        default=32,\n",
    "        type=int,\n",
    "        help='number of records to read during each training step, default=16')\n",
    "    parser.add_argument(\n",
    "        '--window-size',\n",
    "        default=20,\n",
    "        type=int,\n",
    "        help='window size for sliding window in training sample generation, default=100')\n",
    "    parser.add_argument(\n",
    "        '--shift',\n",
    "        default=5,\n",
    "        type=int,\n",
    "        help='shift for sliding window in training sample generation, default=20')\n",
    "    parser.add_argument(\n",
    "        '--stride',\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help='stride inside sliding window in training sample generation, default=1')\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "        default='DEBUG')\n",
    "    parser.add_argument(\n",
    "        '--loss',\n",
    "        default='mean_squared_error',\n",
    "        type=str,\n",
    "        help='loss function used by the model, default=mean_squared_error')\n",
    "    parser.add_argument(\n",
    "        '--shuffle',\n",
    "        default=True,\n",
    "        type=bool,\n",
    "        help='shuffle the batched dataset, default=True'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--shuffle-buffer',\n",
    "        default=5,\n",
    "        type=int,\n",
    "        help='Bigger buffer size means better shuffling but longer setup time. Default=500'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--save-from',\n",
    "        default=80,\n",
    "        type=int,\n",
    "        help='epoch after which model checkpoints are saved, default=80'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--model',\n",
    "        default='split_model',\n",
    "        type=str,\n",
    "        help='The type of model to use, default=\"split_model\", options=\"split_model\", \"full_cnn_model\"'\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def train_and_evaluate(args, tboard_dir, hparams=None):\n",
    "    \"\"\"Trains and evaluates the Keras model.\n",
    "\n",
    "    Uses the Keras model defined in model.py and trains on data loaded and\n",
    "    preprocessed in data_pipeline.py. Saves the trained model in TensorFlow SavedModel\n",
    "    format to the path defined in part by the --job-dir argument.\n",
    "\n",
    "    Args:\n",
    "    args: dictionary of arguments - see get_args() for details\n",
    "    \"\"\"\n",
    "    # Config datasets for consistent usage\n",
    "    ds_config = dict(window_size=args.window_size,\n",
    "                     shift=args.shift,\n",
    "                     stride=args.stride,\n",
    "                     batch_size=args.batch_size)\n",
    "    ds_train_path = args.data_dir_train\n",
    "    ds_val_path = args.data_dir_validate\n",
    "\n",
    "    # create model\n",
    "    if args.model == 'split_model':\n",
    "        print(\"Using split model!\")\n",
    "        model = split_model.create_keras_model(window_size=ds_config[\"window_size\"],\n",
    "                                               loss=args.loss,\n",
    "                                               hparams_config=hparams)\n",
    "    if args.model == 'full_cnn_model':\n",
    "        print(\"Using full cnn model!\")\n",
    "        model = full_cnn_model.create_keras_model(window_size=ds_config[\"window_size\"],\n",
    "                                                  loss=args.loss,\n",
    "                                                  hparams_config=hparams)\n",
    "\n",
    "    # Calculate steps_per_epoch_train, steps_per_epoch_test\n",
    "    # This is needed, since for counting repeat has to be false\n",
    "    steps_per_epoch_train = calculate_steps_per_epoch(data_dir=ds_train_path, dataset_config=ds_config)\n",
    "\n",
    "    steps_per_epoch_validate = calculate_steps_per_epoch(data_dir=ds_val_path, dataset_config=ds_config)\n",
    "\n",
    "    # load datasets\n",
    "    dataset_train = dp.create_dataset(data_dir=ds_train_path,\n",
    "                                      window_size=ds_config[\"window_size\"],\n",
    "                                      shift=ds_config[\"shift\"],\n",
    "                                      stride=ds_config[\"stride\"],\n",
    "                                      batch_size=ds_config[\"batch_size\"])\n",
    "\n",
    "    dataset_validate = dp.create_dataset(data_dir=ds_val_path,\n",
    "                                         window_size=ds_config[\"window_size\"],\n",
    "                                         shift=ds_config[\"shift\"],\n",
    "                                         stride=ds_config[\"stride\"],\n",
    "                                         batch_size=ds_config[\"batch_size\"])\n",
    "\n",
    "    # if hparams is passed, we're running a HPO-job\n",
    "    if hparams:\n",
    "        checkpoint_callback = CustomCheckpoints(save_last_only=True,\n",
    "                                                log_dir=tboard_dir,\n",
    "                                                dataset_path=ds_val_path,\n",
    "                                                dataset_config=ds_config,\n",
    "                                                save_eval_plot=False)\n",
    "    else:\n",
    "        checkpoint_callback = CustomCheckpoints(save_best_only=True,\n",
    "                                                start_epoch=args.save_from,\n",
    "                                                log_dir=tboard_dir,\n",
    "                                                dataset_path=ds_val_path,\n",
    "                                                dataset_config=ds_config,\n",
    "                                                save_eval_plot=False)\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=tboard_dir,\n",
    "                                       histogram_freq=0,\n",
    "                                       write_graph=False,\n",
    "                                       ),\n",
    "        checkpoint_callback,\n",
    "    ]\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # train model\n",
    "    history = model.fit(\n",
    "        dataset_train,\n",
    "        epochs=args.num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch_train,\n",
    "        validation_data=dataset_validate,\n",
    "        validation_steps=steps_per_epoch_validate,\n",
    "        verbose=2,\n",
    "        callbacks=callbacks)\n",
    "\n",
    "    mae_current = min(history.history[\"val_mae_current_cycle\"])\n",
    "    mae_remaining = min(history.history[\"val_mae_remaining_cycles\"])\n",
    "    print('\\nhistory dict:', history.history)\n",
    "    return mae_current, mae_remaining\n",
    "\n",
    "\n",
    "def calculate_steps_per_epoch(data_dir, dataset_config):\n",
    "    temp_dataset = dp.create_dataset(data_dir=data_dir,\n",
    "                                     window_size=dataset_config[\"window_size\"],\n",
    "                                     shift=dataset_config[\"shift\"],\n",
    "                                     stride=dataset_config[\"stride\"],\n",
    "                                     batch_size=dataset_config[\"batch_size\"],\n",
    "                                     repeat=False)\n",
    "    steps_per_epoch = 0\n",
    "    for batch in temp_dataset:\n",
    "        steps_per_epoch += 1\n",
    "    return steps_per_epoch\n",
    "\n",
    "\n",
    "def get_tboard_dir():\n",
    "    run_timestr = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    if args.tboard_dir is None:\n",
    "        tboard_dir = os.path.join(cst.TENSORBOARD_DIR, \"jobs\", run_timestr)\n",
    "    else:\n",
    "        tboard_dir = args.tboard_dir\n",
    "    return tboard_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 启动训练脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using split model!\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Qdlin (InputLayer)              [(None, 20, 1000, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Tdlin (InputLayer)              [(None, 20, 1000, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "detail_concat (Concatenate)     (None, 20, 1000, 2)  0           Qdlin[0][0]                      \n",
      "                                                                 Tdlin[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "convolution (TimeDistributed)   (None, 20, 334, 32)  608         detail_concat[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pool (TimeDistributed)     (None, 20, 167, 32)  0           convolution[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (TimeDistributed)         (None, 20, 56, 64)   18496       conv_pool[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (TimeDistributed)         (None, 20, 28, 64)   0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (TimeDistributed)         (None, 20, 10, 128)  73856       pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (TimeDistributed)         (None, 20, 5, 128)   0           conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "convolution_flat (TimeDistribut (None, 20, 640)      0           pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_cnn (TimeDistributed)   (None, 20, 640)      0           convolution_flat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "IR (InputLayer)                 [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Discharge_time (InputLayer)     [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "QD (InputLayer)                 [(None, 20, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "all_concat (Concatenate)        (None, 20, 643)      0           dropout_cnn[0][0]                \n",
      "                                                                 IR[0][0]                         \n",
      "                                                                 Discharge_time[0][0]             \n",
      "                                                                 QD[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "recurrent (LSTM)                (None, 128)          395264      all_concat[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_lstm (Dropout)          (None, 128)          0           recurrent[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "hidden (Dense)                  (None, 32)           4128        dropout_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 2)            66          hidden[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 492,418\n",
      "Trainable params: 492,418\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "168/168 - 37s - loss: 0.0204 - mae_current_cycle: 240.8860 - mae_remaining_cycles: 182.5198 - val_loss: 0.0310 - val_mae_current_cycle: 325.0843 - val_mae_remaining_cycles: 226.8130\n",
      "Epoch 2/5\n",
      "168/168 - 33s - loss: 0.0176 - mae_current_cycle: 224.5318 - mae_remaining_cycles: 153.9775 - val_loss: 0.0297 - val_mae_current_cycle: 327.5288 - val_mae_remaining_cycles: 181.6377\n",
      "Epoch 3/5\n",
      "168/168 - 32s - loss: 0.0170 - mae_current_cycle: 229.6299 - mae_remaining_cycles: 133.8281 - val_loss: 0.0286 - val_mae_current_cycle: 258.7963 - val_mae_remaining_cycles: 175.5424\n",
      "Epoch 4/5\n",
      "168/168 - 33s - loss: 0.0166 - mae_current_cycle: 226.3160 - mae_remaining_cycles: 133.9464 - val_loss: 0.0260 - val_mae_current_cycle: 302.9955 - val_mae_remaining_cycles: 166.9550\n",
      "Epoch 5/5\n",
      "168/168 - 32s - loss: 0.0166 - mae_current_cycle: 234.1109 - mae_remaining_cycles: 127.3742 - val_loss: 0.0261 - val_mae_current_cycle: 254.5041 - val_mae_remaining_cycles: 154.8972\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:253: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: ['train']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: ['eval']\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.layer.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.layer.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: Graph/jobs/20200603-092546/checkpoints/last_epoch_loss_0.026125840408357002/saved_model.pb\n",
      "\n",
      "history dict: {'loss': [0.020352674811701513, 0.017605139666037366, 0.01696597814970162, 0.016580879891989753, 0.01658461871418348], 'mae_current_cycle': [240.88596, 224.53178, 229.62985, 226.31601, 234.11092], 'mae_remaining_cycles': [182.51978, 153.97751, 133.82812, 133.94643, 127.37415], 'val_loss': [0.031010797427546595, 0.029654115763923492, 0.028625409903695748, 0.026034450000752846, 0.026125840408357002], 'val_mae_current_cycle': [325.0843, 327.5288, 258.79633, 302.99545, 254.50407], 'val_mae_remaining_cycles': [226.81302, 181.63771, 175.54245, 166.95496, 154.8972]}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    logging.set_verbosity(args.verbosity)\n",
    "    train_and_evaluate(args, get_tboard_dir())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型预测\n",
    "### 加载预测及可视化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from trainer.plot import plot_single_prediction\n",
    "from trainer.custom_metrics_losses import mae_current_cycle, mae_remaining_cycles\n",
    "from trainer.clippy import Clippy, clipped_relu\n",
    "\n",
    "def make_plot(model, predictions):\n",
    "    predictions = np.array(predictions)\n",
    "    first_pred = predictions[0]\n",
    "    window_size = model.input_shape[0][1]\n",
    "    # this number comes from training dataset preprocessing\n",
    "    scaling_factors_dict = {\"Remaining_cycles\": 2159.0}\n",
    "    mean_cycle_life = 674  # calculated from training set\n",
    "    figure = plot_single_prediction(first_pred,\n",
    "                                  window_size,\n",
    "                                  scaling_factors_dict,\n",
    "                                  mean_cycle_life)\n",
    "    return figure\n",
    "\n",
    "def make_prediction(model, cycle_data):\n",
    "    cycles = { 'Qdlin': np.array(json.loads(cycle_data['Qdlin'])),\n",
    "                'Tdlin': np.array(json.loads(cycle_data['Tdlin'])),\n",
    "                'IR': np.array(json.loads(cycle_data['IR'])),\n",
    "                'Discharge_time': np.array(json.loads(cycle_data['Discharge_time'])),\n",
    "                'QD': np.array(json.loads(cycle_data['QD']))\n",
    "            }\n",
    "\n",
    "    predictions = model.predict(cycles)\n",
    "    print_predictions = model.predict(cycles)\n",
    "    print_predictions[0] = print_predictions[0] * 2159\n",
    "    print(\"Returning predictions:\")\n",
    "    print(print_predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 导入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = \"saved_model/\"\n",
    "# model = tf.keras.models.load_model(MODEL_DIR, custom_objects={'clippy': Clippy(clipped_relu),\n",
    "#                                                               'mae_current_cycle': mae_current_cycle,\n",
    "#                                                               'mae_remaining_cycles': mae_remaining_cycles})\n",
    "#\n",
    "model = tf.keras.experimental.load_from_saved_model(MODEL_DIR,\n",
    "                                       custom_objects={'clippy': Clippy(clipped_relu),\n",
    "                                                       'mae_current_cycle': mae_current_cycle,\n",
    "                                                       'mae_remaining_cycles': mae_remaining_cycles})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 运用模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning predictions:\n",
      "[[257.15192 605.04913]]\n"
     ]
    }
   ],
   "source": [
    "json_file_sample = './sample_input_4.json'\n",
    "\n",
    "with open(json_file_sample, 'r') as infd:\n",
    "  json_data = json.load(infd)\n",
    "#print(json_data)\n",
    "predictions_soh_rul= make_prediction(model, json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can click the Prediction.html to see the prediction\n"
     ]
    }
   ],
   "source": [
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "figure = make_plot(model, predictions_soh_rul)\n",
    "pyo.plot(figure,filename='Prediction.html')\n",
    "print('You can click the Prediction.html in the left sidebar to see the prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
