{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 数据预处理\n",
    "## 加载数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from trainer.rebuilding_features import load_batches_to_dict\n",
    "import trainer.constants as cst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DropCycleException(Exception):\n",
    "    \"\"\"Used for dropping whole cycles without additional information.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class OutlierException(Exception):\n",
    "    \"\"\"Used for dropping whole cycles based on detected outliers\"\"\"\n",
    "    def __init__(self, message, outlier_dict):\n",
    "        super().__init__(message)\n",
    "        self.outlier_dict = outlier_dict\n",
    "\n",
    "\n",
    "def check_for_drop_warning(array_before, array_after, drop_warning_thresh=0.10):\n",
    "    \"\"\"Checks weather the size of array_after is \"drop_warning_thresh\"-percent\n",
    "    smaller than array_before and issues a warning in that case.\"\"\"\n",
    "\n",
    "    try:\n",
    "        assert float(len(array_before) - len(array_after)) / len(array_before) < drop_warning_thresh, \\\n",
    "            \"\"\"More than {} percent of values were dropped ({} out of {}).\"\"\".format(\n",
    "                drop_warning_thresh * 100,\n",
    "                len(array_before) - len(array_after),\n",
    "                len(array_before))\n",
    "    except AssertionError as e:\n",
    "        warnings.warn(str(e))\n",
    "        # simple_plotly(array_before[-1], V_original=array_before[2])\n",
    "        # simple_plotly(array_after[-1], V_indexed=array_after[2])\n",
    "    finally:\n",
    "        pass\n",
    "\n",
    "\n",
    "def multiple_array_indexing(valid_numpy_index, *args, drop_warning=False, drop_warning_thresh=0.10):\n",
    "    \"\"\"Indexes multiple numpy arrays at once and returns the result in a tuple.\n",
    "\n",
    "    Arguments:\n",
    "        numpy_index {numpy.ndarray or integer sequence} -- The used indeces.\n",
    "\n",
    "    Returns:\n",
    "        tuple -- reindexed numpy arrays from *args in the same order.\n",
    "    \"\"\"\n",
    "    indexed_arrays = [arg[valid_numpy_index].copy() for arg in args]\n",
    "\n",
    "    if drop_warning:\n",
    "        check_for_drop_warning(args[0], indexed_arrays[0])\n",
    "    return tuple(indexed_arrays)\n",
    "\n",
    "\n",
    "def outlier_dict_without_mask(outlier_dict):\n",
    "    \"\"\"Modifies an outlier dict for printing purposes by removing the mask.\n",
    "\n",
    "    Arguments:\n",
    "        outlier_dict {dict} -- Original outliert dictionary.\n",
    "\n",
    "    Returns:\n",
    "        dict -- Same outlier dict without the key \"outliert_mask\"\n",
    "    \"\"\"\n",
    "    outlier_dict_wo_mask = dict()\n",
    "    for key in outlier_dict.keys():\n",
    "        outlier_dict_wo_mask[key] = {k: v for k, v in outlier_dict[key].items() if k != \"outlier_mask\"}\n",
    "    return outlier_dict_wo_mask\n",
    "\n",
    "\n",
    "def compute_outlier_dict(std_multiple_threshold, verbose=False, **kwargs):\n",
    "    \"\"\"Checks for outliers in all numpy arrays given in kwargs by computing the standard deveation of np.diff().\n",
    "    Outliers for every array are defined at the indeces, where the np.diff() is bigger than\n",
    "    std_multiple_threshold times the standard deviation.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        std_multiple_threshold {int} -- Threshold that defines an outlier by multiplying with the\n",
    "            standard deveation (default: {15})\n",
    "        verbose {bool} -- If True, prints the values for every found outlier (default: {False})\n",
    "\n",
    "    Returns:\n",
    "        dict -- The outliert results taged by the names given in kwargs\n",
    "    \"\"\"\n",
    "    outlier_dict = dict()\n",
    "\n",
    "    for key, value in kwargs.items():\n",
    "        diff_values = np.diff(value, prepend=value[0])\n",
    "        std_diff = diff_values.std()\n",
    "        outlier_mask = diff_values > (std_multiple_threshold * std_diff)  # Get the mask for all outliers\n",
    "        outlier_indeces = np.argwhere(outlier_mask)  # Get the indeces for all outliers\n",
    "\n",
    "        if outlier_indeces.size > 0:  # Add outlier information to the outlier dict, if an outlier has been found\n",
    "            outlier_dict[key] = dict(std_diff=std_diff,\n",
    "                                     original_values=value[outlier_indeces],\n",
    "                                     diff_values=diff_values[outlier_indeces],\n",
    "                                     outlier_indeces=outlier_indeces,\n",
    "                                     outlier_mask=outlier_mask)\n",
    "\n",
    "    if verbose and outlier_dict:\n",
    "        # If outlier_dict has any entries, then print a version without the mask (too big for printing)\n",
    "        outlier_dict_wo_mask = outlier_dict_without_mask(outlier_dict)  # Generate a smaller dict for better printing\n",
    "        print(\"############ Found outliers ############ \")\n",
    "        pprint(outlier_dict_wo_mask)\n",
    "        print(\"\")\n",
    "    return outlier_dict\n",
    "\n",
    "\n",
    "def drop_cycle_big_t_outliers(std_multiple_threshold, Qd, T, V, t, t_diff_outlier_thresh=100):\n",
    "    \"\"\"Checks for big outliers in the np.diff() values of t.\n",
    "    If any are found the whole cyce is dropped, with one exception:\n",
    "        There is only one outlier which lays right after the end of discharging.\n",
    "        In this case, all measurement values of Qd, T, V and t after this outlier are dropped and their values returned.\n",
    "\n",
    "        The end of discharging is defined as a V value below 2.01.\n",
    "\n",
    "    Arguments:\n",
    "        outlier_dict {dict} -- Dictionary with outlier information for the whole cycle.\n",
    "        Qd {numpy.ndarray} -- Qd during discharging\n",
    "        T {numpy.ndarray} -- T during discharging\n",
    "        V {numpy.ndarray} -- V during discharging\n",
    "        t {numpy.ndarray} -- t during discharging\n",
    "        t_diff_outlier_thresh {int} -- Threshold that defines what a \"big\" t outliert is\n",
    "\n",
    "    Raises:\n",
    "        OutlierException: Will be raised, if the whole cycle should be dropped.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of numpy.ndarray  -- Returns the original values of Qd, T, V and t if no big t outlier is found, or\n",
    "            a slice of all arrays if the only outlier lays right after the end of discharging.\n",
    "    \"\"\"\n",
    "    outlier_dict = compute_outlier_dict(std_multiple_threshold=std_multiple_threshold, Qd=Qd, T=T, V=V, t=t)\n",
    "    if outlier_dict.get(\"t\"):  # If any outliert was found in t\n",
    "        t_outlier_mask = outlier_dict[\"t\"][\"diff_values\"] > t_diff_outlier_thresh\n",
    "    else:\n",
    "        t_outlier_mask = None\n",
    "\n",
    "    if np.any(t_outlier_mask):  # Only do something if there are big outliers.\n",
    "        # Get the indeces 1 before the t outliers.\n",
    "        indeces_before_t_outliers = outlier_dict[\"t\"][\"outlier_indeces\"][t_outlier_mask] - 1\n",
    "        # Get the minimum V value right before all t outliers.\n",
    "        V_before_t_outlier = np.min(V[indeces_before_t_outliers])\n",
    "\n",
    "        # If there is excatly one t outlier right at the end of discharging,\n",
    "        #   drop all values after this index and continue with processing.\n",
    "        if indeces_before_t_outliers.size == 1 and V_before_t_outlier < 2.01:\n",
    "            i = int(indeces_before_t_outliers) + 1\n",
    "            return Qd[:i], T[:i], V[:i], t[:i]\n",
    "        else:\n",
    "            raise OutlierException(\n",
    "                \"    Dropping cycle based on outliers with np.diff(t) > {} with value(s) {}\".format(\n",
    "                    t_diff_outlier_thresh,\n",
    "                    list(outlier_dict[\"t\"][\"diff_values\"][t_outlier_mask])),\n",
    "                outlier_dict)\n",
    "    else:\n",
    "        return Qd, T, V, t\n",
    "\n",
    "\n",
    "def drop_outliers_starting_left(std_multiple_threshold, Qd, T, V, t):\n",
    "    \"\"\"Searches for outliers in Qd, T, V and t and drops them one by one starting with the smallest index.\n",
    "    Outlier indeces are dropped from every array simultaniously, so the sizes still match.\n",
    "    After the first outliers from every array have been dropped, outliers are computed again, to not drop\n",
    "    false detections.\n",
    "\n",
    "    Arguments:\n",
    "        std_multiple_threshold {int} -- Threshold for the compute_outlier_dict function\n",
    "        Qd {numpy.ndarray} -- Qd measurements\n",
    "        T {numpy.ndarray} -- T measurements\n",
    "        V {numpy.ndarray} -- V measurements\n",
    "        t {numpy.ndarray} -- t measurements\n",
    "\n",
    "    Returns:\n",
    "        tuple of numpy.ndarrays -- All arrays without outliers\n",
    "    \"\"\"\n",
    "    Qd_, T_, V_, t_ = Qd.copy(), T.copy(), V.copy(), t.copy()\n",
    "\n",
    "    # Initialize and compute outliers\n",
    "    drop_counter = 0\n",
    "    outlier_dict = compute_outlier_dict(std_multiple_threshold, verbose=True, Qd=Qd_, T=T_, V=V_, t=t_)\n",
    "    original_outlier_dict = outlier_dict  # copy for debugging und raising OutlierException.\n",
    "\n",
    "    # Process until no outliers are found.\n",
    "    while outlier_dict:\n",
    "        # Get indeces of the left most outlier for every array.\n",
    "        first_outlier_indeces = [np.min(outlier_info[\"outlier_indeces\"]) for outlier_info in outlier_dict.values()]\n",
    "        # Only consider every index once and make it a list type for numpy indexing in array_exclude_index().\n",
    "        unique_indeces_to_drop = list(set(first_outlier_indeces))\n",
    "\n",
    "        # Drop all unique outlier indeces from all arrays.\n",
    "        Qd_ = array_exclude_index(Qd_, unique_indeces_to_drop)\n",
    "        T_ = array_exclude_index(T_, unique_indeces_to_drop)\n",
    "        V_ = array_exclude_index(V_, unique_indeces_to_drop)\n",
    "        t_ = array_exclude_index(t_, unique_indeces_to_drop)\n",
    "\n",
    "        drop_counter += len(unique_indeces_to_drop)\n",
    "\n",
    "        # Recompute outlierts after dropping the unique indeces from all arrays.\n",
    "        outlier_dict = compute_outlier_dict(std_multiple_threshold, Qd=Qd_, T=T_, V=V_, t=t_)\n",
    "\n",
    "    if drop_counter > 0:\n",
    "        print(\"    Dropped {} outliers in {}\".format(drop_counter, list(original_outlier_dict.keys())))\n",
    "        print(\"\")\n",
    "\n",
    "    check_for_drop_warning(Qd, Qd_)\n",
    "    return Qd_, T_, V_, t_\n",
    "\n",
    "\n",
    "def array_exclude_index(arr, id):\n",
    "    \"\"\"Returns the given array without the entry at id.\n",
    "    id can be any valid numpy index.\"\"\"\n",
    "\n",
    "    mask = np.ones_like(arr, bool)\n",
    "    mask[id] = False\n",
    "    return arr[mask]\n",
    "\n",
    "\n",
    "def handle_small_Qd_outliers(std_multiple_threshold, Qd, t, Qd_max_outlier=0.06):\n",
    "    \"\"\"Handles specifically small outliers in Qd, which are a result of constant values for a\n",
    "    small number of measurements before the \"outlier\". The constant values are imputed by linearly interpolating\n",
    "    Qd over t, since Qd over t should be linear anyways. This way the \"outlier\" is \"neutralized\", since there is no\n",
    "    \"step\" left from the constant values to the outlier value.\n",
    "\n",
    "    Arguments:\n",
    "        std_multiple_threshold {int} -- Threshold to use for the compute_outlier_dict function\n",
    "        Qd {numpy.ndarray} -- Qd measurements\n",
    "        t {numpy.ndarray} -- t measurements corresponding to Qd\n",
    "\n",
    "    Keyword Arguments:\n",
    "        Qd_max_outlier {float} -- The maximum absolute value for the found outliers in Qd, which get handled\n",
    "            by this function.\n",
    "        This is needed only to make the function more specific. (default: {0.06})\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray -- The interpolated version of Qd.\n",
    "    \"\"\"\n",
    "\n",
    "    Qd_ = Qd.copy()  # Only copy Qd, since it is the only array values are assigned to\n",
    "    outlier_dict = compute_outlier_dict(std_multiple_threshold, Qd=Qd_)\n",
    "\n",
    "    if outlier_dict.get(\"Qd\"):\n",
    "        # Get only the indeces of all small outliers\n",
    "        small_diff_value_mask = outlier_dict[\"Qd\"][\"diff_values\"] <= Qd_max_outlier\n",
    "        ids = outlier_dict[\"Qd\"][\"outlier_indeces\"][small_diff_value_mask]\n",
    "    else:\n",
    "        ids = None\n",
    "\n",
    "    if ids:\n",
    "        # Interpolate all values before small outliers that stay constant (np.diff == 0)\n",
    "        for i in ids:\n",
    "            # Get the last index, where the value of Qd doesn't stay constant before the outlier.\n",
    "            start_id = int(np.argwhere(np.diff(Qd_[:i]) > 0)[-1])\n",
    "\n",
    "            # Make a mask for where to interpolate\n",
    "            interp_mask = np.zeros_like(Qd_, dtype=bool)\n",
    "            interp_mask[start_id:i] = True\n",
    "            interp_values = np.interp(\n",
    "                t[interp_mask],  # Where to evaluate the interpolation function.\n",
    "                t[~interp_mask],  # X values for the interpolation function.\n",
    "                Qd_[~interp_mask]  # Y values for the interpolation function.\n",
    "            )\n",
    "            # Assign the interpolated values\n",
    "            Qd_[interp_mask] = interp_values\n",
    "            print(\"    Interpolated small Qd outlier from index {} to {}\".format(start_id, i))\n",
    "\n",
    "    return Qd_\n",
    "\n",
    "\n",
    "def make_strictly_decreasing(x_interp, y_interp, prepend_value=3.7):\n",
    "    \"\"\"Takes a monotonically decreasing array y_interp and makes it strictly decreasing by interpolation over x_interp.\n",
    "\n",
    "    Arguments:\n",
    "        x_interp {numpy.ndarray} -- The values to interpolate over.\n",
    "        y_interp {numpy.ndarray} -- Monotonically decreasing values.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        prepend_value {float} -- Value to prepend to y_interp for assesing the difference to the preceding value.\n",
    "            (default: {3.7})\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray -- y_interp with interpolated values, where there used to be zero difference to\n",
    "            the preceding value.\n",
    "    \"\"\"\n",
    "    y_interp_copy = y_interp.copy()\n",
    "    # Make the tale interpolatable if the last value is not the single minimum.\n",
    "    if y_interp_copy[-1] >= y_interp_copy[-2]:\n",
    "        y_interp_copy[-1] -= 0.0001\n",
    "\n",
    "    # Build a mask for all values, which do not decrease.\n",
    "    bigger_equal_zero_diff = np.diff(y_interp_copy, prepend=prepend_value) >= 0\n",
    "    # Replace these values with interpolations based on their border values.\n",
    "    interp_values = np.interp(\n",
    "        x_interp[bigger_equal_zero_diff],  # Where to evaluate the interpolation function.\n",
    "        x_interp[~bigger_equal_zero_diff],  # X values for the interpolation function.\n",
    "        y_interp_copy[~bigger_equal_zero_diff]  # Y values for the interpolation function.\n",
    "    )\n",
    "    y_interp_copy[bigger_equal_zero_diff] = interp_values\n",
    "\n",
    "    # This has to be given, since interpolation will fail otherwise.\n",
    "    assert np.all(np.diff(y_interp_copy) < 0), \"The result y_copy is not strictly decreasing!\"\n",
    "\n",
    "    return y_interp_copy\n",
    "\n",
    "\n",
    "def preprocess_cycle(cycle,\n",
    "                     I_thresh=-3.99,\n",
    "                     Vdlin_start=3.5,\n",
    "                     Vdlin_stop=2.0,\n",
    "                     Vdlin_steps=cst.STEPS,\n",
    "                     return_original_data=False):\n",
    "    \"\"\"Processes data (Qd, T, V, t) from one cycle and resamples Qd, T and V to a predefinded dimension.\n",
    "    discharging_time will be computed based on t and is the only returned feature that is a scalar.\n",
    "\n",
    "    Arguments:\n",
    "        cycle {dict} -- One cycle entry from the original data with keys 'I', 'Qd', 'T', 'V', 't'\n",
    "\n",
    "    Keyword Arguments:\n",
    "        I_thresh {float} -- Only measurements where the current is smaller than this threshold are chosen\n",
    "            (default: {-3.99})\n",
    "        Vdlin_start {float} -- Start value for the resampled V (default: {3.5})\n",
    "        Vdlin_stop {float} -- Stop value for the resampled V (default: {2.0})\n",
    "        Vdlin_steps {int} -- Number of steps V, Qd and T are resampled (default: {1000})\n",
    "        return_original_data {bool} -- Weather the original datapoints, which were used for interpolation,\n",
    "            shold be returned in the results  (default: {False})\n",
    "\n",
    "    Returns:\n",
    "        {dict} -- Dictionary with the resampled (and original) values.\n",
    "    \"\"\"\n",
    "\n",
    "    Qd = cycle[\"Qd\"]\n",
    "    T = cycle[\"T\"]\n",
    "    V = cycle[\"V\"]\n",
    "    I = cycle[\"I\"]  # noqa: E741\n",
    "    t = cycle[\"t\"]\n",
    "\n",
    "    # Only take the measurements during high current discharging.\n",
    "    discharge_mask = I < I_thresh\n",
    "    Qd, T, V, t = multiple_array_indexing(discharge_mask, Qd, T, V, t)\n",
    "\n",
    "    # Sort all values after time.\n",
    "    sorted_indeces = t.argsort()\n",
    "    Qd, T, V, t = multiple_array_indexing(sorted_indeces, Qd, T, V, t)\n",
    "\n",
    "    # Only take timesteps where time is strictly increasing.\n",
    "    increasing_time_mask = np.diff(t, prepend=0) > 0\n",
    "    Qd, T, V, t = multiple_array_indexing(increasing_time_mask, Qd, T, V, t)\n",
    "\n",
    "    # Dropping outliers.\n",
    "    Qd, T, V, t = drop_cycle_big_t_outliers(15, Qd, T, V, t)\n",
    "\n",
    "    Qd = handle_small_Qd_outliers(12, Qd, t)\n",
    "\n",
    "    Qd, T, V, t = drop_outliers_starting_left(12, Qd, T, V, t)\n",
    "\n",
    "    # Apply savitzky golay filter to V to smooth out the values.\n",
    "    # This is done in order to not drop too many values in the next processing step (make monotonically decreasing).\n",
    "    # This way the resulting curves don't become skewed too much in the direction of smaller values.\n",
    "    savgol_window_length = 25\n",
    "    if savgol_window_length >= V.size:\n",
    "        raise DropCycleException(\"\"\"Dropping cycle with less than {} V values.\\nSizes --> Qd:{}, T:{}, V:{}, t:{}\"\"\"\n",
    "                                 .format(savgol_window_length, Qd.size, T.size, V.size, t.size))\n",
    "    V_savgol = savgol_filter(V, window_length=25, polyorder=2)\n",
    "\n",
    "    # Only take the measurements, where V is monotonically decreasing (needed for interpolation).\n",
    "    # This is done by comparing V to the accumulated minimum of V.\n",
    "    #    accumulated minimum --> (taking always the smallest seen value from V from left to right)\n",
    "    v_decreasing_mask = V_savgol == np.minimum.accumulate(V_savgol)\n",
    "    Qd, T, V, t = multiple_array_indexing(v_decreasing_mask, Qd, T, V_savgol, t, drop_warning=True)\n",
    "\n",
    "    # Make V_3 strictly decreasing (needed for interpolation).\n",
    "    V_strict_dec = make_strictly_decreasing(t, V)\n",
    "\n",
    "    # Calculate discharging time. (Only scalar feature which is returned later)\n",
    "    discharging_time = t.max() - t.min()\n",
    "    if discharging_time < 6:\n",
    "        print(\"Test\")\n",
    "        raise DropCycleException(\"Dropping cycle with discharge_time = {}\"\n",
    "                                 .format(discharging_time))\n",
    "\n",
    "    # Make interpolation function.\n",
    "    Qd_interp_func = interp1d(\n",
    "        V_strict_dec[::-1],  # V_strict_dec is inverted because it has to be increasing for interpolation.\n",
    "        Qd[::-1],  # Qd and T are also inverted, so the correct values line up.\n",
    "        bounds_error=False,  # Allows the function to be evaluated outside of the range of V_strict_dec.\n",
    "        fill_value=(Qd[::-1][0], Qd[::-1][-1])  # Values to use, when evaluated outside of V_strict_dec.\n",
    "    )\n",
    "    T_interp_func = interp1d(\n",
    "        V_strict_dec[::-1],\n",
    "        T[::-1],\n",
    "        bounds_error=False,\n",
    "        fill_value=(T[::-1][0], T[::-1][-1])\n",
    "    )\n",
    "\n",
    "    # For resampling the decreasing order is chosen again.\n",
    "    # The order doesn't matter for evaluating Qd_interp_func.\n",
    "    Vdlin = np.linspace(Vdlin_start, Vdlin_stop, Vdlin_steps)\n",
    "\n",
    "    Qdlin = Qd_interp_func(Vdlin)\n",
    "    Tdlin = T_interp_func(Vdlin)\n",
    "\n",
    "    if return_original_data:\n",
    "        return {\n",
    "            cst.QDLIN_NAME: Qdlin,\n",
    "            cst.TDLIN_NAME: Tdlin,\n",
    "            cst.VDLIN_NAME: Vdlin,\n",
    "            cst.DISCHARGE_TIME_NAME: discharging_time,\n",
    "            # Original data used for interpolation.\n",
    "            \"Qd_original_data\": Qd,\n",
    "            \"T_original_data\": T,\n",
    "            \"V_original_data\": V,\n",
    "            \"t_original_data\": t\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            cst.QDLIN_NAME: Qdlin,\n",
    "            cst.TDLIN_NAME: Tdlin,\n",
    "            cst.VDLIN_NAME: Vdlin,\n",
    "            cst.DISCHARGE_TIME_NAME: discharging_time\n",
    "        }\n",
    "\n",
    "\n",
    "def preprocess_batch(batch_dict, return_original_data=False, return_cycle_drop_info=False, verbose=False):\n",
    "    \"\"\"Processes all cycles of every cell in batch_dict and returns the results in the same format.\n",
    "\n",
    "    Arguments:\n",
    "        batch_dict {dict} -- Unprocessed batch of cell data.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        return_original_data {bool} -- If True, the original data used for interpolation is returned. (default: {False})\n",
    "        verbose {bool} -- If True prints progress for every cell (default: {False})\n",
    "\n",
    "    Returns:\n",
    "        dict -- Results in the same format as batch_dict.\n",
    "    \"\"\"\n",
    "    batch_results = dict()\n",
    "    cycles_drop_info = dict()\n",
    "\n",
    "    print(\"Start processing data ...\")\n",
    "\n",
    "    for cell_key in list(batch_dict.keys()):\n",
    "        # The iteration is over a list of keys so the processed keys can be removed while iterating over the dict.\n",
    "        # This reduces the memory used during processing.\n",
    "        # If \"for cell_key, cell_value in batch_dict.items()\" is used,\n",
    "        #    \"del batch_dict[cell_key]\" would throw an RuntimeError: dictionary changed size during iteration.\n",
    "        cell_value = batch_dict[cell_key]\n",
    "        # Initialite the cell results with all available scalar values.\n",
    "        batch_results[cell_key] = dict(\n",
    "            cycle_life=cell_value[\"cycle_life\"][0][0],\n",
    "            summary={\n",
    "                cst.INTERNAL_RESISTANCE_NAME: [],\n",
    "                cst.QD_NAME: [],\n",
    "                cst.REMAINING_CYCLES_NAME: [],\n",
    "                cst.DISCHARGE_TIME_NAME: []\n",
    "            },\n",
    "            cycles=dict()\n",
    "        )\n",
    "\n",
    "        for cycle_key, cycle_value in cell_value[\"cycles\"].items():\n",
    "            # Has to be skipped since there are often times only two measurements.\n",
    "            if cycle_key == '0':\n",
    "                continue\n",
    "            # Some cells have more cycle measurements than recorded cycle_life.\n",
    "            # The reamining cycles will be dropped.\n",
    "            elif int(cycle_key) > int(cell_value[\"cycle_life\"][0][0]):\n",
    "                print(\"    Cell {} has more cycles than cycle_life ({}): Dropping remaining cycles {} to {}\"\n",
    "                      .format(cell_key,\n",
    "                              cell_value[\"cycle_life\"][0][0],\n",
    "                              cycle_key,\n",
    "                              max([int(k) for k in cell_value[\"cycles\"].keys()])))\n",
    "                break\n",
    "\n",
    "            # Start processing the cycle.\n",
    "            try:\n",
    "                cycle_results = preprocess_cycle(cycle_value, return_original_data=return_original_data)\n",
    "\n",
    "            except DropCycleException as e:\n",
    "                print(\"cell:\", cell_key, \" cycle:\", cycle_key)\n",
    "                print(e)\n",
    "                print(\"\")\n",
    "                # Documenting dropped cell and key\n",
    "                drop_info = {cell_key: {cycle_key: None}}\n",
    "                cycles_drop_info.update(drop_info)\n",
    "                continue\n",
    "\n",
    "            except OutlierException as oe:  # Can be raised if preprocess_cycle, if an outlier is found.\n",
    "                print(\"cell:\", cell_key, \" cycle:\", cycle_key)\n",
    "                print(oe)\n",
    "                print(\"\")\n",
    "                # Adding outlier dict from Exception to the cycles_drop_info.\n",
    "                drop_info = {\n",
    "                    cell_key: {\n",
    "                        cycle_key: outlier_dict_without_mask(oe.outlier_dict)}}\n",
    "                cycles_drop_info.update(drop_info)\n",
    "                continue\n",
    "\n",
    "            # Copy summary values for this cycle into the results.\n",
    "            # I tried writing it into an initialized array, but then indeces of dropped cycles get skipped.\n",
    "            batch_results[cell_key][\"summary\"][cst.INTERNAL_RESISTANCE_NAME].append(\n",
    "                cell_value[\"summary\"][cst.INTERNAL_RESISTANCE_NAME][int(cycle_key)])\n",
    "            batch_results[cell_key][\"summary\"][cst.QD_NAME].append(\n",
    "                cell_value[\"summary\"][cst.QD_NAME][int(cycle_key)])\n",
    "            batch_results[cell_key][\"summary\"][cst.REMAINING_CYCLES_NAME].append(\n",
    "                cell_value[\"cycle_life\"][0][0] - int(cycle_key))\n",
    "\n",
    "            # Append the calculated discharge time.\n",
    "            # This is the only scalar results from preprocess_cycle\n",
    "            batch_results[cell_key][\"summary\"][cst.DISCHARGE_TIME_NAME].append(\n",
    "                cycle_results.pop(cst.DISCHARGE_TIME_NAME))\n",
    "\n",
    "            # Write the results to the correct cycle key.\n",
    "            batch_results[cell_key][\"cycles\"][cycle_key] = cycle_results\n",
    "\n",
    "        # Convert lists of appended values to numpy arrays.\n",
    "        for k, v in batch_results[cell_key][\"summary\"].items():\n",
    "            batch_results[cell_key][\"summary\"][k] = np.array(v)\n",
    "\n",
    "        if verbose:\n",
    "            print(cell_key, \"done\")\n",
    "        # Delete cell key from dict, to reduce used memory during processing.\n",
    "        del batch_dict[cell_key]\n",
    "\n",
    "    cycles_drop_info[\"number_distinct_cells\"] = len(cycles_drop_info)\n",
    "    cycles_drop_info[\"number_distinct_cycles\"] = sum([len(value) for key, value in cycles_drop_info.items()\n",
    "                                                      if key != \"number_distinct_cells\"])\n",
    "\n",
    "    print(\"Done processing data.\")\n",
    "    if return_cycle_drop_info:\n",
    "        return batch_results, cycles_drop_info\n",
    "    else:\n",
    "        return batch_results\n",
    "\n",
    "\n",
    "def describe_results_dict(results_dict):\n",
    "    \"\"\"Prints summary statistics for all computed results over every single cycle.\n",
    "    This might take a few seconds, since it has to search the whole dictionary for every vallue.\"\"\"\n",
    "    print(\"Collecting results data and printing results ...\")\n",
    "    describe_dict = dict()\n",
    "\n",
    "    cycle_life_list = [cell[\"cycle_life\"] for cell in results_dict.values()]\n",
    "\n",
    "    describe_dict.update(dict(\n",
    "        cycle_life=dict(\n",
    "            max=np.max(cycle_life_list),\n",
    "            min=np.min(cycle_life_list),\n",
    "            mean=np.mean(cycle_life_list),\n",
    "            std=np.std(cycle_life_list)\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    summary_results = dict()\n",
    "    for k in [cst.INTERNAL_RESISTANCE_NAME,\n",
    "              cst.QD_NAME,\n",
    "              cst.REMAINING_CYCLES_NAME,\n",
    "              cst.DISCHARGE_TIME_NAME]:\n",
    "        summary_results[k] = dict(\n",
    "            max=np.max([np.max(cell[\"summary\"][k]) for cell in results_dict.values()]),\n",
    "            min=np.min([np.min(cell[\"summary\"][k]) for cell in results_dict.values()]),\n",
    "            mean=np.mean([np.mean(cell[\"summary\"][k]) for cell in results_dict.values()]),\n",
    "            mean_std=np.std([np.std(cell[\"summary\"][k]) for cell in results_dict.values()])\n",
    "        )\n",
    "    describe_dict.update(dict(summary_results=summary_results))\n",
    "\n",
    "    cycle_results = dict()\n",
    "    for k in [cst.QDLIN_NAME, cst.TDLIN_NAME]:\n",
    "        cycle_results[k] = dict(\n",
    "            max=np.max([np.max(cycle[k]) for cell in results_dict.values() for cycle in cell[\"cycles\"].values()]),\n",
    "            min=np.min([np.min(cycle[k]) for cell in results_dict.values() for cycle in cell[\"cycles\"].values()]),\n",
    "            mean=np.mean([np.mean(cycle[k]) for cell in results_dict.values() for cycle in cell[\"cycles\"].values()]),\n",
    "            mean_std=np.mean([np.std(cycle[k]) for cell in results_dict.values() for cycle in cell[\"cycles\"].values()])\n",
    "        )\n",
    "    describe_dict.update(dict(cycle_results=cycle_results))\n",
    "\n",
    "    pprint(describe_dict)\n",
    "\n",
    "\n",
    "def save_preprocessed_data(results_dict, save_dir=cst.PROCESSED_DATA):\n",
    "    print(\"Saving preprocessed data to {}\".format(save_dir))\n",
    "    with open(save_dir, 'wb') as f:\n",
    "        pickle.dump(results_dict, f)\n",
    "\n",
    "\n",
    "def load_preprocessed_data(save_dir=cst.PROCESSED_DATA):\n",
    "    print(\"Loading preprocessed data from {}\".format(save_dir))\n",
    "    with open(save_dir, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def data_processing_main():\n",
    "    batch_dict = load_batches_to_dict(amount_to_load=3)\n",
    "\n",
    "    results, cycles_drop_info = preprocess_batch(batch_dict,\n",
    "                                                 return_original_data=False,\n",
    "                                                 return_cycle_drop_info=True,\n",
    "                                                 verbose=True)\n",
    "\n",
    "    pprint(cycles_drop_info)\n",
    "    # describe_results_dict(results)\n",
    "\n",
    "    save_preprocessed_data(results)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 启动数据预处理脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading batch1 ...\n",
      "Loading batch2 ...\n",
      "Loading batch3 ...\n",
      "Done loading batches\n",
      "Start processing data ...\n",
      "    Interpolated small Qd outlier from index 34 to 42\n",
      "b1c0 done\n",
      "b1c1 done\n",
      "cell: b1c2  cycle: 1486\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [488.0221083333337]\n",
      "\n",
      "b1c2 done\n",
      "cell: b1c3  cycle: 11\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [407.7868733333333]\n",
      "\n",
      "b1c3 done\n",
      "cell: b1c4  cycle: 11\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [407.7970250000001]\n",
      "\n",
      "    Interpolated small Qd outlier from index 176 to 186\n",
      "b1c4 done\n",
      "cell: b1c5  cycle: 908\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [264.66089166666694]\n",
      "\n",
      "b1c5 done\n",
      "    Interpolated small Qd outlier from index 75 to 84\n",
      "b1c6 done\n",
      "    Interpolated small Qd outlier from index 84 to 92\n",
      "b1c7 done\n",
      "b1c9 done\n",
      "    Interpolated small Qd outlier from index 152 to 162\n",
      "b1c11 done\n",
      "    Interpolated small Qd outlier from index 182 to 191\n",
      "b1c14 done\n",
      "    Interpolated small Qd outlier from index 167 to 175\n",
      "b1c15 done\n",
      "b1c16 done\n",
      "    Interpolated small Qd outlier from index 206 to 214\n",
      "b1c17 done\n",
      "b1c18 done\n",
      "b1c19 done\n",
      "cell: b1c20  cycle: 12\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [408.8027033333334]\n",
      "\n",
      "    Interpolated small Qd outlier from index 126 to 135\n",
      "b1c20 done\n",
      "cell: b1c21  cycle: 12\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [408.7915833333333]\n",
      "\n",
      "    Interpolated small Qd outlier from index 202 to 210\n",
      "b1c21 done\n",
      "    Interpolated small Qd outlier from index 117 to 125\n",
      "    Interpolated small Qd outlier from index 92 to 101\n",
      "b1c23 done\n",
      "    Interpolated small Qd outlier from index 211 to 221\n",
      "b1c24 done\n",
      "b1c25 done\n",
      "b1c26 done\n",
      "b1c27 done\n",
      "b1c28 done\n",
      "b1c29 done\n",
      "    Interpolated small Qd outlier from index 199 to 207\n",
      "    Interpolated small Qd outlier from index 45 to 55\n",
      "b1c30 done\n",
      "    Interpolated small Qd outlier from index 136 to 144\n",
      "b1c31 done\n",
      "    Interpolated small Qd outlier from index 206 to 214\n",
      "    Interpolated small Qd outlier from index 52 to 61\n",
      "b1c32 done\n",
      "    Interpolated small Qd outlier from index 191 to 199\n",
      "    Interpolated small Qd outlier from index 55 to 63\n",
      "b1c33 done\n",
      "b1c34 done\n",
      "    Interpolated small Qd outlier from index 69 to 77\n",
      "    Interpolated small Qd outlier from index 37 to 45\n",
      "b1c35 done\n",
      "############ Found outliers ############ \n",
      "{'V': {'diff_values': array([[0.3274812]]),\n",
      "       'original_values': array([[3.4858503]]),\n",
      "       'outlier_indeces': array([[65]]),\n",
      "       'std_diff': 0.025368069589014473}}\n",
      "\n",
      "    Dropped 2 outliers in ['V']\n",
      "\n",
      "b1c36 done\n",
      "    Interpolated small Qd outlier from index 214 to 223\n",
      "b1c37 done\n",
      "    Interpolated small Qd outlier from index 88 to 97\n",
      "b1c38 done\n",
      "    Interpolated small Qd outlier from index 94 to 102\n",
      "b1c39 done\n",
      "    Interpolated small Qd outlier from index 198 to 207\n",
      "b1c40 done\n",
      "    Interpolated small Qd outlier from index 190 to 198\n",
      "b1c41 done\n",
      "b1c42 done\n",
      "b1c43 done\n",
      "    Interpolated small Qd outlier from index 132 to 141\n",
      "b1c44 done\n",
      "    Interpolated small Qd outlier from index 36 to 44\n",
      "b1c45 done\n",
      "    Cell b2c0 has more cycles than cycle_life (300.0): Dropping remaining cycles 301 to 325\n",
      "b2c0 done\n",
      "    Cell b2c1 has more cycles than cycle_life (148.0): Dropping remaining cycles 149 to 169\n",
      "b2c1 done\n",
      "    Cell b2c2 has more cycles than cycle_life (438.0): Dropping remaining cycles 439 to 463\n",
      "b2c2 done\n",
      "    Cell b2c3 has more cycles than cycle_life (335.0): Dropping remaining cycles 336 to 360\n",
      "b2c3 done\n",
      "cell: b2c4  cycle: 246\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [488.8776733333332]\n",
      "\n",
      "    Cell b2c4 has more cycles than cycle_life (444.0): Dropping remaining cycles 445 to 470\n",
      "b2c4 done\n",
      "    Cell b2c5 has more cycles than cycle_life (480.0): Dropping remaining cycles 481 to 506\n",
      "b2c5 done\n",
      "cell: b2c6  cycle: 257\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [488.9175416666675]\n",
      "\n",
      "    Cell b2c6 has more cycles than cycle_life (511.0): Dropping remaining cycles 512 to 544\n",
      "b2c6 done\n",
      "cell: b2c10  cycle: 250\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [489.02010166666685]\n",
      "\n",
      "    Cell b2c10 has more cycles than cycle_life (561.0): Dropping remaining cycles 562 to 595\n",
      "b2c10 done\n",
      "cell: b2c11  cycle: 249\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [489.6243683333334]\n",
      "\n",
      "    Cell b2c11 has more cycles than cycle_life (477.0): Dropping remaining cycles 478 to 504\n",
      "b2c11 done\n",
      "    Cell b2c12 has more cycles than cycle_life (458.0): Dropping remaining cycles 459 to 490\n",
      "b2c12 done\n",
      "    Cell b2c13 has more cycles than cycle_life (483.0): Dropping remaining cycles 484 to 519\n",
      "b2c13 done\n",
      "    Cell b2c14 has more cycles than cycle_life (485.0): Dropping remaining cycles 486 to 508\n",
      "b2c14 done\n",
      "    Cell b2c17 has more cycles than cycle_life (494.0): Dropping remaining cycles 495 to 524\n",
      "b2c17 done\n",
      "    Cell b2c18 has more cycles than cycle_life (487.0): Dropping remaining cycles 488 to 522\n",
      "b2c18 done\n",
      "    Cell b2c19 has more cycles than cycle_life (461.0): Dropping remaining cycles 462 to 488\n",
      "b2c19 done\n",
      "cell: b2c20  cycle: 248\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [489.2605916666672]\n",
      "\n",
      "    Cell b2c20 has more cycles than cycle_life (502.0): Dropping remaining cycles 503 to 530\n",
      "b2c20 done\n",
      "    Cell b2c21 has more cycles than cycle_life (489.0): Dropping remaining cycles 490 to 519\n",
      "b2c21 done\n",
      "cell: b2c22  cycle: 246\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [489.37804500000004]\n",
      "\n",
      "    Cell b2c22 has more cycles than cycle_life (513.0): Dropping remaining cycles 514 to 539\n",
      "b2c22 done\n",
      "    Cell b2c23 has more cycles than cycle_life (527.0): Dropping remaining cycles 528 to 561\n",
      "b2c23 done\n",
      "    Cell b2c24 has more cycles than cycle_life (495.0): Dropping remaining cycles 496 to 523\n",
      "b2c24 done\n",
      "    Cell b2c25 has more cycles than cycle_life (461.0): Dropping remaining cycles 462 to 486\n",
      "b2c25 done\n",
      "    Cell b2c26 has more cycles than cycle_life (471.0): Dropping remaining cycles 472 to 497\n",
      "b2c26 done\n",
      "cell: b2c27  cycle: 118\n",
      "Dropping cycle with less than 25 V values.\n",
      "Sizes --> Qd:4, T:4, V:4, t:4\n",
      "\n",
      "    Cell b2c27 has more cycles than cycle_life (468.0): Dropping remaining cycles 469 to 494\n",
      "b2c27 done\n",
      "cell: b2c28  cycle: 249\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [489.5417833333331]\n",
      "\n",
      "    Cell b2c28 has more cycles than cycle_life (509.0): Dropping remaining cycles 510 to 540\n",
      "b2c28 done\n",
      "    Cell b2c29 has more cycles than cycle_life (498.0): Dropping remaining cycles 499 to 521\n",
      "b2c29 done\n",
      "    Cell b2c30 has more cycles than cycle_life (481.0): Dropping remaining cycles 482 to 506\n",
      "b2c30 done\n",
      "    Cell b2c31 has more cycles than cycle_life (492.0): Dropping remaining cycles 493 to 518\n",
      "b2c31 done\n",
      "    Cell b2c32 has more cycles than cycle_life (519.0): Dropping remaining cycles 520 to 543\n",
      "b2c32 done\n",
      "    Cell b2c33 has more cycles than cycle_life (520.0): Dropping remaining cycles 521 to 545\n",
      "b2c33 done\n",
      "    Cell b2c34 has more cycles than cycle_life (499.0): Dropping remaining cycles 500 to 525\n",
      "b2c34 done\n",
      "    Cell b2c35 has more cycles than cycle_life (463.0): Dropping remaining cycles 464 to 485\n",
      "b2c35 done\n",
      "############ Found outliers ############ \n",
      "{'V': {'diff_values': array([[0.1490802]]),\n",
      "       'original_values': array([[2.1538448]]),\n",
      "       'outlier_indeces': array([[287]]),\n",
      "       'std_diff': 0.010335980567599382}}\n",
      "\n",
      "    Dropped 5 outliers in ['V']\n",
      "\n",
      "    Cell b2c36 has more cycles than cycle_life (535.0): Dropping remaining cycles 536 to 559\n",
      "b2c36 done\n",
      "cell: b2c37  cycle: 246\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [490.5330916666668]\n",
      "\n",
      "    Cell b2c37 has more cycles than cycle_life (478.0): Dropping remaining cycles 479 to 501\n",
      "b2c37 done\n",
      "    Cell b2c38 has more cycles than cycle_life (465.0): Dropping remaining cycles 466 to 486\n",
      "b2c38 done\n",
      "    Cell b2c39 has more cycles than cycle_life (459.0): Dropping remaining cycles 460 to 479\n",
      "b2c39 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (226 out of 650).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (176 out of 649).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (155 out of 567).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (116 out of 576).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (123 out of 470).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (70 out of 542).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (85 out of 569).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (42 out of 387).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (138 out of 584).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (120 out of 532).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (228 out of 683).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (135 out of 560).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (70 out of 459).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (53 out of 450).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (56 out of 485).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (52 out of 451).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (132 out of 600).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (75 out of 511).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (78 out of 491).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (57 out of 445).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (168 out of 681).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (187 out of 683).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (212 out of 706).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (115 out of 542).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (63 out of 453).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (143 out of 564).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (75 out of 407).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (117 out of 486).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (230 out of 679).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (188 out of 616).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (116 out of 539).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (103 out of 572).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (73 out of 485).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (131 out of 620).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (181 out of 654).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (135 out of 607).\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (59 out of 391).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Cell b2c40 has more cycles than cycle_life (499.0): Dropping remaining cycles 500 to 529\n",
      "b2c40 done\n",
      "    Cell b2c41 has more cycles than cycle_life (429.0): Dropping remaining cycles 430 to 451\n",
      "b2c41 done\n",
      "cell: b2c42  cycle: 246\n",
      "    Dropping cycle based on outliers with np.diff(t) > 100 with value(s) [490.9387683333335]\n",
      "\n",
      "    Cell b2c42 has more cycles than cycle_life (466.0): Dropping remaining cycles 467 to 491\n",
      "b2c42 done\n",
      "    Cell b2c43 has more cycles than cycle_life (462.0): Dropping remaining cycles 463 to 486\n",
      "b2c43 done\n",
      "    Cell b2c44 has more cycles than cycle_life (457.0): Dropping remaining cycles 458 to 478\n",
      "b2c44 done\n",
      "    Cell b2c45 has more cycles than cycle_life (487.0): Dropping remaining cycles 488 to 513\n",
      "b2c45 done\n",
      "    Cell b2c46 has more cycles than cycle_life (429.0): Dropping remaining cycles 430 to 448\n",
      "b2c46 done\n",
      "    Cell b2c47 has more cycles than cycle_life (713.0): Dropping remaining cycles 714 to 744\n",
      "b2c47 done\n",
      "b3c0 done\n",
      "b3c1 done\n",
      "b3c3 done\n",
      "b3c4 done\n",
      "b3c5 done\n",
      "b3c6 done\n",
      "b3c7 done\n",
      "b3c8 done\n",
      "b3c9 done\n",
      "b3c10 done\n",
      "b3c11 done\n",
      "b3c12 done\n",
      "b3c13 done\n",
      "b3c14 done\n",
      "b3c15 done\n",
      "b3c16 done\n",
      "b3c17 done\n",
      "b3c18 done\n",
      "b3c19 done\n",
      "b3c20 done\n",
      "b3c21 done\n",
      "############ Found outliers ############ \n",
      "{'t': {'diff_values': array([[11.333155]]),\n",
      "       'original_values': array([[21.462045]]),\n",
      "       'outlier_indeces': array([[1]]),\n",
      "       'std_diff': 0.727404702389029}}\n",
      "\n",
      "    Dropped 99 outliers in ['t']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (99 out of 240).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3c22 done\n",
      "b3c24 done\n",
      "b3c25 done\n",
      "b3c26 done\n",
      "b3c27 done\n",
      "b3c28 done\n",
      "b3c29 done\n",
      "b3c30 done\n",
      "b3c31 done\n",
      "b3c33 done\n",
      "b3c34 done\n",
      "b3c35 done\n",
      "b3c36 done\n",
      "b3c40 done\n",
      "############ Found outliers ############ \n",
      "{'t': {'diff_values': array([[11.43943]]),\n",
      "       'original_values': array([[21.80653333]]),\n",
      "       'outlier_indeces': array([[1]]),\n",
      "       'std_diff': 0.7327046437363047}}\n",
      "\n",
      "    Dropped 100 outliers in ['t']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:23: UserWarning: More than 10.0 percent of values were dropped (100 out of 241).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3c41 done\n",
      "b3c42 done\n",
      "b3c43 done\n",
      "b3c44 done\n",
      "b3c45 done\n",
      "Done processing data.\n",
      "{'b1c2': {'1486': {'t': {'diff_values': array([[488.02210833]]),\n",
      "                         'original_values': array([[522.817995]]),\n",
      "                         'outlier_indeces': array([[71]]),\n",
      "                         'std_diff': 26.8623624050339}}},\n",
      " 'b1c20': {'12': {'t': {'diff_values': array([[408.80270333]]),\n",
      "                        'original_values': array([[439.54111667]]),\n",
      "                        'outlier_indeces': array([[33]]),\n",
      "                        'std_diff': 21.849172126874798}}},\n",
      " 'b1c21': {'12': {'t': {'diff_values': array([[408.79158333]]),\n",
      "                        'original_values': array([[439.59233167]]),\n",
      "                        'outlier_indeces': array([[34]]),\n",
      "                        'std_diff': 21.78640787927781}}},\n",
      " 'b1c3': {'11': {'V': {'diff_values': array([[0.1669254]]),\n",
      "                       'original_values': array([[3.1514935]]),\n",
      "                       'outlier_indeces': array([[218]]),\n",
      "                       'std_diff': 0.010333896762920769},\n",
      "                 't': {'diff_values': array([[407.78687333]]),\n",
      "                       'original_values': array([[452.83853333]]),\n",
      "                       'outlier_indeces': array([[218]]),\n",
      "                       'std_diff': 21.794821830254392}}},\n",
      " 'b1c4': {'11': {'V': {'diff_values': array([[0.1687146]]),\n",
      "                       'original_values': array([[3.1403642]]),\n",
      "                       'outlier_indeces': array([[219]]),\n",
      "                       'std_diff': 0.010378803350227296},\n",
      "                 't': {'diff_values': array([[407.797025]]),\n",
      "                       'original_values': array([[452.84387167]]),\n",
      "                       'outlier_indeces': array([[219]]),\n",
      "                       'std_diff': 21.73337858146465}}},\n",
      " 'b1c5': {'908': {'V': {'diff_values': array([[0.1552573]]),\n",
      "                        'original_values': array([[3.1849294]]),\n",
      "                        'outlier_indeces': array([[166]]),\n",
      "                        'std_diff': 0.01031481613838876},\n",
      "                  't': {'diff_values': array([[264.66089167]]),\n",
      "                        'original_values': array([[299.884755]]),\n",
      "                        'outlier_indeces': array([[166]]),\n",
      "                        'std_diff': 14.981075811711168}}},\n",
      " 'b2c10': {'250': {'t': {'diff_values': array([[489.02010167]]),\n",
      "                         'original_values': array([[531.03066]]),\n",
      "                         'outlier_indeces': array([[133]]),\n",
      "                         'std_diff': 27.040443251881864}}},\n",
      " 'b2c11': {'249': {'t': {'diff_values': array([[489.62436833]]),\n",
      "                         'original_values': array([[533.54968333]]),\n",
      "                         'outlier_indeces': array([[159]]),\n",
      "                         'std_diff': 27.240968207012514}}},\n",
      " 'b2c20': {'248': {'t': {'diff_values': array([[489.26059167]]),\n",
      "                         'original_values': array([[530.72455167]]),\n",
      "                         'outlier_indeces': array([[119]]),\n",
      "                         'std_diff': 27.09514779776072}}},\n",
      " 'b2c22': {'246': {'V': {'diff_values': array([[0.5064803]]),\n",
      "                         'original_values': array([[3.1088758]]),\n",
      "                         'outlier_indeces': array([[242]]),\n",
      "                         'std_diff': 0.028183073941087607},\n",
      "                   't': {'diff_values': array([[489.378045]]),\n",
      "                         'original_values': array([[537.63218]]),\n",
      "                         'outlier_indeces': array([[242]]),\n",
      "                         'std_diff': 25.613088373972413}}},\n",
      " 'b2c27': {'118': None},\n",
      " 'b2c28': {'249': {'t': {'diff_values': array([[489.54178333]]),\n",
      "                         'original_values': array([[530.94081167]]),\n",
      "                         'outlier_indeces': array([[127]]),\n",
      "                         'std_diff': 26.824331825816568}}},\n",
      " 'b2c37': {'246': {'V': {'diff_values': array([[0.1489649]]),\n",
      "                         'original_values': array([[3.195035]]),\n",
      "                         'outlier_indeces': array([[167]]),\n",
      "                         'std_diff': 0.009823425596577909},\n",
      "                   't': {'diff_values': array([[490.53309167]]),\n",
      "                         'original_values': array([[534.260045]]),\n",
      "                         'outlier_indeces': array([[167]]),\n",
      "                         'std_diff': 26.838392305832418}}},\n",
      " 'b2c4': {'246': {'V': {'diff_values': array([[0.1817274]]),\n",
      "                        'original_values': array([[3.2009821]]),\n",
      "                        'outlier_indeces': array([[165]]),\n",
      "                        'std_diff': 0.01136635349529337},\n",
      "                  't': {'diff_values': array([[488.87767333]]),\n",
      "                        'original_values': array([[532.86243167]]),\n",
      "                        'outlier_indeces': array([[165]]),\n",
      "                        'std_diff': 26.788029044787134}}},\n",
      " 'b2c42': {'246': {'V': {'diff_values': array([[0.1877776]]),\n",
      "                         'original_values': array([[3.1858354]]),\n",
      "                         'outlier_indeces': array([[177]]),\n",
      "                         'std_diff': 0.011790954205435513},\n",
      "                   't': {'diff_values': array([[490.93876833]]),\n",
      "                         'original_values': array([[536.22061833]]),\n",
      "                         'outlier_indeces': array([[177]]),\n",
      "                         'std_diff': 27.105068066337747}}},\n",
      " 'b2c6': {'257': {'V': {'diff_values': array([[0.7192716]]),\n",
      "                        'original_values': array([[3.0178761]]),\n",
      "                        'outlier_indeces': array([[264]]),\n",
      "                        'std_diff': 0.03801681805532508},\n",
      "                  't': {'diff_values': array([[488.91754167]]),\n",
      "                        'original_values': array([[534.949625]]),\n",
      "                        'outlier_indeces': array([[264]]),\n",
      "                        'std_diff': 25.41563440612858}}},\n",
      " 'number_distinct_cells': 16,\n",
      " 'number_distinct_cycles': 16}\n",
      "Saving preprocessed data to data/processed_data.pkl\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data_processing_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 数据 PIPLINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 加载训练数据生成函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.train import FloatList, Feature, Features, Example\n",
    "\n",
    "import trainer.constants as cst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_cycle_example(cell_value, summary_idx, cycle_idx, scaling_factors):\n",
    "    \"\"\"\n",
    "    Define the columns that should be written to tfrecords and converts the raw data\n",
    "    to \"Example\" objects. Every Example contains data from one charging cycle.\n",
    "    The data is scaled (divided) by the corresponding values in \"scaling_factors\".\n",
    "    \"\"\"\n",
    "    # Summary feature values (scalars --> have to be wrapped in lists)\n",
    "    ir_value = [cell_value[\"summary\"][cst.INTERNAL_RESISTANCE_NAME][summary_idx]\n",
    "                / scaling_factors[cst.INTERNAL_RESISTANCE_NAME]]\n",
    "    qd_value = [cell_value[\"summary\"][cst.QD_NAME][summary_idx]\n",
    "                / scaling_factors[cst.QD_NAME]]\n",
    "    rc_value = [cell_value[\"summary\"][cst.REMAINING_CYCLES_NAME][summary_idx]\n",
    "                / scaling_factors[cst.REMAINING_CYCLES_NAME]]\n",
    "    dt_value = [cell_value[\"summary\"][cst.DISCHARGE_TIME_NAME][summary_idx]\n",
    "                / scaling_factors[cst.DISCHARGE_TIME_NAME]]\n",
    "    cc_value = [float(cycle_idx)\n",
    "                / scaling_factors[cst.REMAINING_CYCLES_NAME]]  # Same scale --> same scaling factor\n",
    "\n",
    "    # Detail feature values (arrays)\n",
    "    qdlin_value = cell_value[\"cycles\"][cycle_idx][cst.QDLIN_NAME] / scaling_factors[cst.QDLIN_NAME]\n",
    "    tdlin_value = cell_value[\"cycles\"][cycle_idx][cst.TDLIN_NAME] / scaling_factors[cst.TDLIN_NAME]\n",
    "\n",
    "    # Wrapping as example\n",
    "    cycle_example = Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                cst.INTERNAL_RESISTANCE_NAME:\n",
    "                    Feature(float_list=FloatList(value=ir_value)),\n",
    "                cst.QD_NAME:\n",
    "                    Feature(float_list=FloatList(value=qd_value)),\n",
    "                cst.REMAINING_CYCLES_NAME:\n",
    "                    Feature(float_list=FloatList(value=rc_value)),\n",
    "                cst.DISCHARGE_TIME_NAME:\n",
    "                    Feature(float_list=FloatList(value=dt_value)),\n",
    "                cst.QDLIN_NAME:\n",
    "                    Feature(float_list=FloatList(value=qdlin_value)),\n",
    "                cst.TDLIN_NAME:\n",
    "                    Feature(float_list=FloatList(value=tdlin_value)),\n",
    "                cst.CURRENT_CYCLE_NAME:\n",
    "                    Feature(float_list=FloatList(value=cc_value))\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return cycle_example\n",
    "\n",
    "\n",
    "def write_to_tfrecords(batteries, data_dir, train_test_split=None):\n",
    "    \"\"\"\n",
    "    Takes battery data in dict format as input and writes a set of tfrecords files to disk.\n",
    "\n",
    "    To load the preprocessed battery data that was used to train the model, use the\n",
    "    \"load_processed_battery_data()\" function and pass it as the batteries argument to the\n",
    "    \"Write_to_tfrecords()\" function.\n",
    "\n",
    "    A train/test split can be passed as a dictionary with the names of the splits (e.g. \"train\") as keys\n",
    "    and lists of cell names (e.g. [\"b1c3\", \"b1c4\"]) as values. This will create subdirectories for each\n",
    "    split.\n",
    "\n",
    "    For more info on TFRecords and Examples see 'Hands-on Machine Learning with\n",
    "    Scikit-Learn, Keras & TensorFlow', pp.416 (2nd edition, early release)\n",
    "    \"\"\"\n",
    "    # Create base directory for tfrecords\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "\n",
    "    scaling_factors = calculate_and_save_scaling_factors(batteries, train_test_split, cst.SCALING_FACTORS_DIR)\n",
    "\n",
    "    if train_test_split is None:\n",
    "        # Write all cells into one directory\n",
    "        for cell_name, cell_data in batteries.items():\n",
    "            write_single_cell(cell_name, cell_data, data_dir, scaling_factors)\n",
    "    else:\n",
    "        # For each split set a new working directory in /Data/tfrecords\n",
    "        # and write files there\n",
    "        for split_name, split_indexes in train_test_split.items():\n",
    "            split_data_dir = os.path.join(data_dir, split_name)\n",
    "            # create directories\n",
    "            if not os.path.exists(split_data_dir):\n",
    "                os.mkdir(split_data_dir)\n",
    "            split_batteries = {idx: batteries[idx] for idx in split_indexes}\n",
    "            for cell_name, cell_data in split_batteries.items():\n",
    "                write_single_cell(cell_name, cell_data, split_data_dir, scaling_factors)\n",
    "\n",
    "\n",
    "def write_single_cell(cell_name, cell_data, data_dir, scaling_factors):\n",
    "    \"\"\"\n",
    "    Takes data for one cell and writes it to a tfrecords file with the naming convention\n",
    "    \"b1c0.tfrecord\". The SerializeToString() method creates binary data out of the\n",
    "    Example objects that can be read natively in TensorFlow.\n",
    "    \"\"\"\n",
    "    filename = os.path.join(data_dir, cell_name + \".tfrecord\")\n",
    "    with tf.io.TFRecordWriter(str(filename)) as f:\n",
    "        for summary_idx, cycle_idx in enumerate(cell_data[\"cycles\"].keys()):\n",
    "            cycle_to_write = get_cycle_example(cell_data, summary_idx, cycle_idx, scaling_factors)\n",
    "            f.write(cycle_to_write.SerializeToString())\n",
    "    print(\"Created %s.tfrecords file.\" % cell_name)\n",
    "\n",
    "\n",
    "def parse_features(example_proto):\n",
    "    \"\"\"\n",
    "    The parse_features function takes an example and converts it from binary/message format\n",
    "    into a more readable format. To be able to feed the dataset directly into a\n",
    "    Tensorflow model later on, we split the data into examples and targets (i.e. X and y).\n",
    "\n",
    "    The feature_description defines the schema/specifications to read from TFRecords.\n",
    "    This could also be done by declaring feature columns and parsing the schema\n",
    "    with tensorflow.feature_columns.make_parse_example_spec().\n",
    "    \"\"\"\n",
    "    feature_description = {\n",
    "        cst.INTERNAL_RESISTANCE_NAME: tf.io.FixedLenFeature([1, ], tf.float32),\n",
    "        cst.QD_NAME: tf.io.FixedLenFeature([1, ], tf.float32),\n",
    "        cst.DISCHARGE_TIME_NAME: tf.io.FixedLenFeature([1, ], tf.float32),\n",
    "        cst.REMAINING_CYCLES_NAME: tf.io.FixedLenFeature([], tf.float32),\n",
    "        cst.CURRENT_CYCLE_NAME: tf.io.FixedLenFeature([], tf.float32),\n",
    "        cst.TDLIN_NAME: tf.io.FixedLenFeature([cst.STEPS, cst.INPUT_DIM], tf.float32),\n",
    "        cst.QDLIN_NAME: tf.io.FixedLenFeature([cst.STEPS, cst.INPUT_DIM], tf.float32)\n",
    "    }\n",
    "    examples = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    target_remaining = examples.pop(cst.REMAINING_CYCLES_NAME)\n",
    "    target_current = examples.pop(cst.CURRENT_CYCLE_NAME)\n",
    "    targets = tf.stack([target_current, target_remaining], 0)\n",
    "\n",
    "    return examples, targets\n",
    "\n",
    "\n",
    "def get_flatten_windows(window_size):\n",
    "    def flatten_windows(features, target):\n",
    "        \"\"\"\n",
    "        Calling .window() on our dataset created a dataset of type \"VariantDataset\"\n",
    "        for every feature in our main dataset. We need to flatten\n",
    "        these VariantDatasets before we can feed everything to a model.\n",
    "        Because the VariantDataset are modeled after windows, they have\n",
    "        length=window_size.\n",
    "        \"\"\"\n",
    "        # Select all rows for each feature\n",
    "        qdlin = features[cst.QDLIN_NAME].batch(window_size)\n",
    "        tdlin = features[cst.TDLIN_NAME].batch(window_size)\n",
    "        ir = features[cst.INTERNAL_RESISTANCE_NAME].batch(window_size)\n",
    "        dc_time = features[cst.DISCHARGE_TIME_NAME].batch(window_size)\n",
    "        qd = features[cst.QD_NAME].batch(window_size)\n",
    "        # the names in this dict have to match the names of the Input objects in\n",
    "        # our final model\n",
    "        features_flat = {\n",
    "            cst.QDLIN_NAME: qdlin,\n",
    "            cst.TDLIN_NAME: tdlin,\n",
    "            cst.INTERNAL_RESISTANCE_NAME: ir,\n",
    "            cst.DISCHARGE_TIME_NAME: dc_time,\n",
    "            cst.QD_NAME: qd\n",
    "        }\n",
    "        # For every window we want to have one target/label\n",
    "        # so we only get the last row by skipping all but one row\n",
    "        target_flat = target.skip(window_size - 1)\n",
    "        return tf.data.Dataset.zip((features_flat, target_flat))\n",
    "    return flatten_windows\n",
    "\n",
    "\n",
    "def get_create_cell_dataset_from_tfrecords(window_size, shift, stride, drop_remainder):\n",
    "    def create_cell_dataset_from_tfrecords(file):\n",
    "        \"\"\"\n",
    "        The read_tfrecords() function reads a file, skipping the first row which in our case\n",
    "        is 0/NaN most of the time. It then loops over each example/row in the dataset and\n",
    "        calls the parse_feature function. Then it batches the dataset, so it always feeds\n",
    "        multiple examples at the same time, then shuffles the batches. It is important\n",
    "        that we batch before shuffling, so the examples within the batches stay in order.\n",
    "        \"\"\"\n",
    "        dataset = tf.data.TFRecordDataset(file)\n",
    "        dataset = dataset.map(parse_features)\n",
    "        dataset = dataset.window(size=window_size, shift=shift, stride=stride, drop_remainder=drop_remainder)\n",
    "        dataset = dataset.flat_map(get_flatten_windows(window_size))\n",
    "        return dataset\n",
    "    return create_cell_dataset_from_tfrecords\n",
    "\n",
    "\n",
    "def create_dataset(data_dir, window_size, shift, stride, batch_size,\n",
    "                   cycle_length=4, num_parallel_calls=4,\n",
    "                   drop_remainder=True, shuffle=True,\n",
    "                   shuffle_buffer=500, repeat=True):\n",
    "    \"\"\"\n",
    "    Creates a dataset from .tfrecord files in the data directory. Expects a regular expression\n",
    "    to capture multiple files (e.g. \"data/tfrecords/train/*tfrecord\").\n",
    "    The dataset will augment the original data by creating windows of loading cycles.\n",
    "\n",
    "    To load unprocessed data, set \"preprocessed\" to False.\n",
    "\n",
    "    Notes about the interleave() method:\n",
    "    interleave() will create a dataset that pulls 4 (=cycle_length) file paths from the\n",
    "    filepath_dataset and for each one calls the function \"read_tfrecords()\". It will then\n",
    "    cycle through these 4 datasets, reading one line at a time from each until all datasets\n",
    "    are out of items. Then it gets the next 4 file paths from the filepath_dataset and\n",
    "    interleaves them the same way, and so on until it runs out of file paths.\n",
    "    Even with parallel calls specified, data within batches is sequential.\n",
    "    \"\"\"\n",
    "    filepath_dataset = tf.data.Dataset.list_files(data_dir)\n",
    "    assembled_dataset = filepath_dataset.interleave(get_create_cell_dataset_from_tfrecords(window_size, shift, stride,\n",
    "                                                                                           drop_remainder),\n",
    "                                                    cycle_length=cycle_length,\n",
    "                                                    num_parallel_calls=num_parallel_calls)\n",
    "    if shuffle:\n",
    "        assembled_dataset = assembled_dataset.shuffle(shuffle_buffer)\n",
    "\n",
    "    # The batching has to happen after shuffling the windows, so one batch is not sequential\n",
    "    assembled_dataset = assembled_dataset.batch(batch_size)\n",
    "\n",
    "    if repeat:\n",
    "        assembled_dataset = assembled_dataset.repeat()\n",
    "    return assembled_dataset\n",
    "\n",
    "\n",
    "def calculate_and_save_scaling_factors(data_dict, train_test_split, csv_dir):\n",
    "    \"\"\"Calculates the scaling factors for every feature based on the training set in train_test_split\n",
    "    and saves the result in a csv file. The factors are used during writing of the tfrecords files.\"\"\"\n",
    "\n",
    "    print(\"Calculate scaling factors...\")\n",
    "    scaling_factors = dict()\n",
    "\n",
    "    if train_test_split != None:\n",
    "        # only take training cells\n",
    "        data_dict = {k: v for k, v in data_dict.items() if k in train_test_split[\"train\"]}\n",
    "    else:\n",
    "        # only take non-secondary-test cells\n",
    "        data_dict = {k: v for k, v in data_dict.items() if k.startswith('b3')}\n",
    "\n",
    "\n",
    "    # Calculating max values for summary features\n",
    "    for k in [cst.INTERNAL_RESISTANCE_NAME,\n",
    "              cst.QD_NAME,\n",
    "              cst.REMAINING_CYCLES_NAME,  # The feature \"Current_cycles\" will be scaled by the same scaling factor\n",
    "              cst.DISCHARGE_TIME_NAME]:\n",
    "        # Two max() calls are needed, one for every cell, one over all cells\n",
    "        scaling_factors[k] = max([max(cell_v[\"summary\"][k])\n",
    "                                  for cell_k, cell_v in data_dict.items()\n",
    "                                  for cycle_v in cell_v[\"cycles\"].values()])\n",
    "\n",
    "    # Calculating max values for detail features\n",
    "    for k in [cst.QDLIN_NAME,\n",
    "              cst.TDLIN_NAME]:\n",
    "        # Two max() calls are needed, one over every cycle array, one over all cycles (all cells included)\n",
    "        scaling_factors[k] = max([max(cycle_v[k])\n",
    "                                  for cell_k, cell_v in data_dict.items()\n",
    "                                  for cycle_v in cell_v[\"cycles\"].values()])\n",
    "\n",
    "    with open(csv_dir, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=scaling_factors.keys())\n",
    "        writer.writeheader()  # Write the field names in the first line of the csv\n",
    "        writer.writerow(scaling_factors)  # Write values to the corrent fields\n",
    "    print(\"Saved scaling factors to {}\".format(csv_dir))\n",
    "    print(\"Scaling factors: {}\".format(scaling_factors))\n",
    "    return scaling_factors\n",
    "\n",
    "\n",
    "def load_scaling_factors(csv_dir=cst.SCALING_FACTORS_DIR, gcloud_bucket=None):\n",
    "    \"\"\"Reads the scaling factors from a csv and returns them as a dict.\"\"\"\n",
    "    if gcloud_bucket:\n",
    "        blob = gcloud_bucket.blob(csv_dir)\n",
    "        names, values = blob.download_as_string().decode(\"utf-8\").split(\"\\r\\n\")[:2]  # Download and decode byte string.\n",
    "        return {k: float(v) for k, v in zip(names.split(\",\"), values.split(\",\"))}\n",
    "    else:\n",
    "        with open(csv_dir, mode='r') as file:\n",
    "            csv_reader = csv.DictReader(file)\n",
    "            for row in csv_reader:\n",
    "                return {k: float(v) for k, v in row.items()}  # Return only the first found line with numeric values\n",
    "\n",
    "\n",
    "# dev method\n",
    "def load_train_test_split():\n",
    "    \"\"\"\n",
    "    Loads a train_test_split dict that divides all cell names into three lists,\n",
    "    recreating the splits from the original paper.\n",
    "    This can be passed directly to \"write_to_tfrecords()\" as an argument.\n",
    "    \"\"\"\n",
    "    return pickle.load(open(cst.TRAIN_TEST_SPLIT, \"rb\"))\n",
    "\n",
    "\n",
    "# dev method\n",
    "def load_processed_battery_data():\n",
    "    return pickle.load(open(cst.PROCESSED_DATA, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  启动训练数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing datasets with train/test split from original paper and preprocessed data.\n",
      "Loading split...\n",
      "Loading battery data...\n",
      "Start writing to disk...\n",
      "Calculate scaling factors...\n",
      "Saved scaling factors to data/tfrecords/scaling_factors.csv\n",
      "Scaling factors: {'IR': 0.022039292, 'QD': 1.0979686, 'Remaining_cycles': 2159.0, 'Discharge_time': 14.758193333333232, 'Qdlin': 1.0828815, 'Tdlin': 41.78867160669522}\n",
      "Created b1c1.tfrecords file.\n",
      "Created b1c3.tfrecords file.\n",
      "Created b1c5.tfrecords file.\n",
      "Created b1c7.tfrecords file.\n",
      "Created b1c11.tfrecords file.\n",
      "Created b1c15.tfrecords file.\n",
      "Created b1c17.tfrecords file.\n",
      "Created b1c19.tfrecords file.\n",
      "Created b1c21.tfrecords file.\n",
      "Created b1c24.tfrecords file.\n",
      "Created b1c26.tfrecords file.\n",
      "Created b1c28.tfrecords file.\n",
      "Created b1c30.tfrecords file.\n",
      "Created b1c32.tfrecords file.\n",
      "Created b1c34.tfrecords file.\n",
      "Created b1c36.tfrecords file.\n",
      "Created b1c38.tfrecords file.\n",
      "Created b1c40.tfrecords file.\n",
      "Created b1c42.tfrecords file.\n",
      "Created b1c44.tfrecords file.\n",
      "Created b2c0.tfrecords file.\n",
      "Created b2c2.tfrecords file.\n",
      "Created b2c4.tfrecords file.\n",
      "Created b2c6.tfrecords file.\n",
      "Created b2c11.tfrecords file.\n",
      "Created b2c13.tfrecords file.\n",
      "Created b2c17.tfrecords file.\n",
      "Created b2c19.tfrecords file.\n",
      "Created b2c21.tfrecords file.\n",
      "Created b2c23.tfrecords file.\n",
      "Created b2c25.tfrecords file.\n",
      "Created b2c27.tfrecords file.\n",
      "Created b2c29.tfrecords file.\n",
      "Created b2c31.tfrecords file.\n",
      "Created b2c33.tfrecords file.\n",
      "Created b2c35.tfrecords file.\n",
      "Created b2c37.tfrecords file.\n",
      "Created b2c39.tfrecords file.\n",
      "Created b2c41.tfrecords file.\n",
      "Created b2c43.tfrecords file.\n",
      "Created b2c45.tfrecords file.\n",
      "Created b1c0.tfrecords file.\n",
      "Created b1c2.tfrecords file.\n",
      "Created b1c4.tfrecords file.\n",
      "Created b1c6.tfrecords file.\n",
      "Created b1c9.tfrecords file.\n",
      "Created b1c14.tfrecords file.\n",
      "Created b1c16.tfrecords file.\n",
      "Created b1c18.tfrecords file.\n",
      "Created b1c20.tfrecords file.\n",
      "Created b1c23.tfrecords file.\n",
      "Created b1c25.tfrecords file.\n",
      "Created b1c27.tfrecords file.\n",
      "Created b1c29.tfrecords file.\n",
      "Created b1c31.tfrecords file.\n",
      "Created b1c33.tfrecords file.\n",
      "Created b1c35.tfrecords file.\n",
      "Created b1c37.tfrecords file.\n",
      "Created b1c39.tfrecords file.\n",
      "Created b1c41.tfrecords file.\n",
      "Created b1c43.tfrecords file.\n",
      "Created b1c45.tfrecords file.\n",
      "Created b2c1.tfrecords file.\n",
      "Created b2c3.tfrecords file.\n",
      "Created b2c5.tfrecords file.\n",
      "Created b2c10.tfrecords file.\n",
      "Created b2c12.tfrecords file.\n",
      "Created b2c14.tfrecords file.\n",
      "Created b2c18.tfrecords file.\n",
      "Created b2c20.tfrecords file.\n",
      "Created b2c22.tfrecords file.\n",
      "Created b2c24.tfrecords file.\n",
      "Created b2c26.tfrecords file.\n",
      "Created b2c28.tfrecords file.\n",
      "Created b2c30.tfrecords file.\n",
      "Created b2c32.tfrecords file.\n",
      "Created b2c34.tfrecords file.\n",
      "Created b2c36.tfrecords file.\n",
      "Created b2c38.tfrecords file.\n",
      "Created b2c40.tfrecords file.\n",
      "Created b2c42.tfrecords file.\n",
      "Created b2c44.tfrecords file.\n",
      "Created b2c46.tfrecords file.\n",
      "Created b2c47.tfrecords file.\n",
      "Created b3c0.tfrecords file.\n",
      "Created b3c1.tfrecords file.\n",
      "Created b3c3.tfrecords file.\n",
      "Created b3c4.tfrecords file.\n",
      "Created b3c5.tfrecords file.\n",
      "Created b3c6.tfrecords file.\n",
      "Created b3c7.tfrecords file.\n",
      "Created b3c8.tfrecords file.\n",
      "Created b3c9.tfrecords file.\n",
      "Created b3c10.tfrecords file.\n",
      "Created b3c11.tfrecords file.\n",
      "Created b3c12.tfrecords file.\n",
      "Created b3c13.tfrecords file.\n",
      "Created b3c14.tfrecords file.\n",
      "Created b3c15.tfrecords file.\n",
      "Created b3c16.tfrecords file.\n",
      "Created b3c17.tfrecords file.\n",
      "Created b3c18.tfrecords file.\n",
      "Created b3c19.tfrecords file.\n",
      "Created b3c20.tfrecords file.\n",
      "Created b3c21.tfrecords file.\n",
      "Created b3c22.tfrecords file.\n",
      "Created b3c24.tfrecords file.\n",
      "Created b3c25.tfrecords file.\n",
      "Created b3c26.tfrecords file.\n",
      "Created b3c27.tfrecords file.\n",
      "Created b3c28.tfrecords file.\n",
      "Created b3c29.tfrecords file.\n",
      "Created b3c30.tfrecords file.\n",
      "Created b3c31.tfrecords file.\n",
      "Created b3c33.tfrecords file.\n",
      "Created b3c34.tfrecords file.\n",
      "Created b3c35.tfrecords file.\n",
      "Created b3c36.tfrecords file.\n",
      "Created b3c40.tfrecords file.\n",
      "Created b3c41.tfrecords file.\n",
      "Created b3c42.tfrecords file.\n",
      "Created b3c43.tfrecords file.\n",
      "Created b3c44.tfrecords file.\n",
      "Created b3c45.tfrecords file.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Writing datasets with train/test split from original paper and preprocessed data.\")\n",
    "    print(\"Loading split...\")\n",
    "    split = load_train_test_split()\n",
    "    print(\"Loading battery data...\")\n",
    "    battery_data = load_processed_battery_data()\n",
    "    print(\"Start writing to disk...\")\n",
    "    write_to_tfrecords(battery_data, cst.DATASETS_DIR, train_test_split=split)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
